{"./":{"url":"./","title":"Introduction","keywords":"","body":"Notes Notes for my readings and thinkings. Copyright © Niu 2015 ~ 2018 all right reserved，powered by GitbookUpdate Time: 2018-05-22 20:50:34 "},"esl-reading-notes.html":{"url":"esl-reading-notes.html","title":"ESL Reading Notes","keywords":"","body":"Elements of Statistical Learning elements of statistical learning 是上学的时候就开始看的一本书，刚开始读的非常吃力，很多概念或者公式不是非常理解。现在已经工作一年了，断断续续还在看这本书，明显能够感觉到对不熟悉的算法的理解深刻了许多，但是这本书给我带来的知识并没有在实际应用中有较多的体现，继续前行吧。 Copyright © Niu 2015 ~ 2018 all right reserved，powered by GitbookUpdate Time: 2018-05-22 20:50:34 "},"esl-reading-notes/chapter-2.html":{"url":"esl-reading-notes/chapter-2.html","title":"Chapter 2","keywords":"","body":"Overview of Supervised Learning 本章主要是概论章节，讲述了统计学习的基础概念与问题。 2.2 变量类型 变量的类型：定序 定类 定距 定比 2.3 最小二乘与KKK邻近 最小二乘法的假设更加严格，而结果更加稳定，但是偏差可能比较大。 K邻近的假设非常少，结果比较准确（随着k的变小，结果越来越精确），但是其稳定性较差（随着k的升高，越来越稳定） 2.3.1 最小二乘法 最小二乘法做分类时更适合数据来源于Bivariate Gaussian分布，并且相互不相关，拥有不同的均值（此情形之后称为情景一） 2.3.2 K邻近 K邻近算法用目标x周围最近的k个x的函数值的平均来表示目标x的值。 Y^(x)=1k∑xi∈Nk(x)yi\\hat{Y}(x)=\\frac{1}{k}\\sum_{x_i \\in N_k(x)}y_i​Y​^​​(x)=​k​​1​​∑​x​i​​∈N​k​​(x)​​y​i​​ 通过邻近的这个概念是一种距离的测度，常用的距离为欧几里得距离。 K邻近的方法更适用于数据来源比较复杂的情况，例如来源于10个混合的低方差高斯分布，其中每个高斯分布的均值来源于同一个高斯分布(此情形之后称为情景二)。 K邻近方法需要计算的有效参数值个数大多数情况下比最小二乘法多，而且不能使用损失函数来惩罚。 k的大小有一个trade-off在里面，虽然k越小越精确，但是稳定性会越来越差，所以要选择合适的k。 2.3.3 最小二乘与K邻近的比较 一个小的总结： 最小二乘的方法得到的decision boundary是非常平滑的，它依赖于比较强的假设条件，得到的结果一般来讲variance较小，而bias比较高。更适用于情景一。 K邻近方法不需要数据的强假设，可是适用于任何情形，虽然其得到的结果非常精确，既Bias较低，但是其variance一般是比较大的。更适用于情景二。 2.4 统计决策理论 什么是最优的预测值？ 2.4.1 输出是连续变量的情况 如果使用MSE作为Loss Function的话，那么最好的预测就是基于观测值的条件期望。 Nearest-neighbor的方法更直接，它有两个近似： 1.用平均值替代均值 2.用目标x周围的邻近点来替代条件期望。 随着样本量N的增加，目标x的邻近点离x越来越近。随着k的增加，均值也越来越稳定。也就是说随着N和k的增加，K邻近的预测值将趋近于利用MSE作为损失函数的条件期望。 虽然如此，但是随着dimension p的增加，收敛依旧成立，其收敛速度会随着p的增加而减慢。 线性回归的方法直接将regression function近似为线性的。 虽然k邻近和最小二乘方法都是利用平均化的思想来估计条件期望，但是他们的假设却别非常大： 最小二乘法假设f(x)总体是一个线性的方程 k邻近假设f(x)又一个局部的常数方程决定 2.4.2 输出为分类变量的情况 在输出变为离散的情况下，我们定义一个Loss Function来惩罚分类出现的错误。再对所有类的Loss Function的求期望，使得这个期望最小的分类就是所估计的分类结果。 通过求解发现，结果很容易理解为，在给定了输入的情况下，使得出现类的概率最大的一类即所估计的类。这个分类方法就是Bayes classifier，而bayes classifier分类的错误率就成为bayes rate。 对于k邻近的方法，与连续的情况类似，利用平均值来近似期望， 利用x周围k个邻近点来近似条件希望。 2.5 高维下的局部方法 虽然KNN方法在样本量增大的情况下有着较好的逼近性质，但是在高维情况下也会有许多问题出现。 高维情况下，局部的方法将不怎么局部。 r1/pr^{1/p}r​1/p​​ 高维情况下，样本点更趋向于集中于样本的边界。 高维情况下，实际的样本将变得非常稀疏。 2.6 统计模型、监督学习、函数估计 2.4节我们已经知道了，通过最小化Loss Function的期望来估计模型函数f(x)，常见的有最小二乘法和KNN，但是这种方法也有一定的缺陷： 如果是在高维情况下，KNN方法与估计点的距离不如预想中的近（2.5节 第二种情形） 如果输入的数据有特殊的结构存在，那么就可以同时降低偏差和方差。 2.6.1 统计模型 Additive error model Y=f(X)+ϵY=f(X)+\\epsilonY=f(X)+ϵ 随机误差项ϵ\\epsilonϵ是零均值的，而且与XXX独立。 这个模型适用于输出为连续型变量，而对于离散型（定性变量）更加直接，f(X)f(X)f(X)直接用Pr(G∥X)Pr(G\\|X)Pr(G∥X)代替即可。 2.6.2 监督学习 Supervised learning attempts to learn fff by example through a teacher.(ESL page 29) 2.6.3 函数估计 最大似然估计 2.7 结构化回归 可以看出局部方法有诸多缺陷，为了解决这些问题，出现了结构化回归方法。 2.7.1 方法的难点 考虑到RSS条件： RSS(f)=∑i=1N(yi−f(xi))2 RSS(f)=\\sum_{i=1}^{N}(y_i-f(x_i))^2RSS(f)=∑​i=1​N​​(y​i​​−f(x​i​​))​2​​ 只要经过点(xi,yi)(x_i,y_i)(x​i​​,y​i​​)的f(x)f(x)f(x)都是RSS的一个解，可见这种解有无数多个，为了确定一个更小范围的解，就需要多加一些限制条件。 没有哪一种解法是最优的解法，要根据不同的情况来自己决定。 通常来说，所有解法都是基于同一个思想，对于距离x足够小的范围内的点实行同一种规则，例如近似常数、线性或者低阶多项式等等。 限制的力度取决于neighborhood size，规模越大，约束的能力越强，同时也表明该方法对不同的约束会更敏感。 [?] 限制的性质一方面也取决于所使用的metric。可以简单分为explicitly和implicitly两种。 有一点必须知道的是，如果在任何各项同性的领域内使用可变的方程，那么同样会产生高维的问题。并且，所有克服了高维问题的方法基本上都不允许领域在各个维度上同时变小。 2.8 约束的方法分类 2.7节所说的限制的种类大致可以分为几个类，而大多数方法都可以归结在这几类或这几类的组合中。 2.8.1 添加惩罚项的方法 这种方法有时候也称为正则化方法。 2.8.2 核函数方法 核方程对于目标x周围的x给予不同的权重。 2.8.3 基函数方法 这个是对线性模型的推广，假设模型仍然是可加的，对系数而言仍然是线性的，但是所加的这些项则变成了函数（基函数），对这种方法而言，比较常用的一个是样条，另外一个是多项式回归。 2.9 模型选择 模型选择要基于test sample，主要是在bias与variance的tradeoff中选择一个度，使得expected prediction error达到最小。 Copyright © Niu 2015 ~ 2018 all right reserved，powered by GitbookUpdate Time: 2018-05-22 20:50:34 "},"esl-reading-notes/chapter-3.html":{"url":"esl-reading-notes/chapter-3.html","title":"Chapter 3","keywords":"","body":"Linear Regression Models and Least SquaresIntroduction Copyright © Niu 2015 ~ 2018 all right reserved，powered by GitbookUpdate Time: 2018-05-22 20:50:34 "},"shu-ju-wa-jue-dao-lun.html":{"url":"shu-ju-wa-jue-dao-lun.html","title":"数据挖掘导论","keywords":"","body":"数据挖掘导论是一本介绍型的入门书籍，比较好的解释了基础的机器学习算法，是一本纯理论的书籍，相比较推荐较多的「机器学习实战」、「集体智慧编程」等等来说缺少coding的训练。 本书主要包含5个大方面的内容： 数据 分类方法 关联分析 聚类方法 异常值检测 数据是基础章节，主要介绍了数据的类型、基础变换方法及描述方法等。 剩下的章节无前后顺序，可以按照自己的重点学习，如果只是入门建议按照分类 -> 聚类 -> 关联分析 -> 异常值检测的顺序阅读。 Copyright © Niu 2015 ~ 2018 all right reserved，powered by GitbookUpdate Time: 2018-05-22 20:50:34 "},"shu-ju-wa-jue-dao-lun/shu-ju.html":{"url":"shu-ju-wa-jue-dao-lun/shu-ju.html","title":"数据","keywords":"","body":"1.概述 数据挖掘已然是当下做数据的人不能不知道的知识了，那么数据挖掘到底是什么呢。个人理解就是在数据中发现有价值的信息。数据挖掘的任务大概可以分为预测和描述两大类。预测包括我们常说的回归与分类，前者针对于目标变量是连续的情况，而后者针对目标变量是离散的情况。描述是做什么呢，就是概括数据中潜在的联系模式，比如相关性、趋势、聚类、异常等等。常用的技术包括分类、聚类、关联分析、异常检测等。 2.数据 说数据挖掘的技术之前，先得说说数据。巧妇难为无米之炊，数据对于数据分析人员的重要性不言而喻。罗老师讲过，数据分析就像做菜一样，首先得有材料（数据），然后还得洗菜（数据清洗、探索），然后得有菜谱（统计、机器学习模型），然后选择对的菜谱做菜（构建模型），最后摆摆盘（数据可视化，生成分析报告），再就是洗盘子了（规整资料）。作为基础，对于数据质量、类型的了解，对其处理的方式以及可用的模型是一个数据分析员具备的基本知识。 首先就是我们统计一开始就会学到的数据的类型：定类、定序、定距、定比。不多说了。而对于每种类型数据常用的方法可以归纳为： 类型 方法 变换方法 定类 众数、熵，列联相关 一对一变换 定序 中位数、百分位数、秩相关、游程检验、符号检验 单调变换 定距 均值、方差、皮尔逊相关系数、t和F检验 正彷射变换 定比 几何平均数（环比）、调和平均数（相对变化率）、百分比变差 scalar 同时数据按照不同的分类标准也可以分为：连续数据、离散数据。对称数据、非对称数据（文档词频矩阵）。 一般在观察一个数据集的时候考虑三个方面的特性：维度（变量个数），稀疏性，分辨率。通过这三个方面来了解这个数据集的基本特性。 下面说说数据质量的问题，数据的质量关乎整个数据分析结构的优劣，数据的质量主要由这几个方面构成： 测量误差和数据收集错误 测量误差在一定程度上是无法避免的，有可能是系统的也有可能是随机的。收集错误，一般都可以很好的纠正。 噪声和伪像 噪声是测量误差的随机部分，无法避免。一个算法的鲁棒性就是来衡量这个算法是否能应对噪声的干扰。数据错误或者确定性的失真就是伪像。 精度、偏倚、准确率 离群点 也称为异常值，离群点与噪声不同，是人们感兴趣的对象，如：信用卡欺诈。 缺失值 缺失值常用的处理方法： 删除数据或者属性；估计缺失值（众数、中位数、有分布的随机抽样）；忽略缺失值。 为了让数据更适合模型，为了改善模型、减少时间、降低成本、提高质量，常常需要对数据进行预处理。 根据需要处理的数据规模，对碎片化的数据进行规整。例如我只需要年销售额的数据，而只有销售额的数据，就需要事先进行聚集。 抽样。 选取具有代表性的样本。 常用的抽样方法有：简单随机抽样、分层抽样、整群抽样、有序抽样等等。 确定正确的样本容量是一个麻烦事。可以采用渐进抽样的方法（可以理解为逐步试验），逐步增加样本容量，观察结果的准确性，当当本容量的增加使得结果准确定的增幅小于一个可以接受的阈值时，即可以停止，确定样本容量了。 降维。Curse of Dimensionality听着名字就觉得太可怕了。降维也是近代统计研究的一个重要方向。在数据预处理的阶段我们就可以开始着手处理这个问题了。降低了维度首先会提高算法的性能，再者更容易理解（人对抽象空间的理解能力有限），而且可以方便与可视化。最最常用的线性技术就是大名鼎鼎的PCA了，还有他的好兄弟SVD。针对时间序列的数据，莫过于傅里叶变换了，以及小波变换。 还有一种方法就要对数据的含义有很好的理解了，这就需要这个领域的专家才可以。比如人为的特征提取，处理一个问题，专家觉得这个问题的主要影响方面有哪几个就把哪几个变量作为输入。还有新特征的构造，也需要专家的指点，比如区分一堆金属，我们有他们的质量与体积，这两个变量不好利用，我们就创建一个新的变量：密度，这就是特征的构造。 离散化。对于很多分类的模型，他们的目标变量往往是连续的，如何正确的将连续变量映射到离散空间也是有学问的。如果是非监督的方法，我们常用的就是分个区间，把对应的数标记为这个区间的类型，区间的划分主要有：等宽方法，等频率方法，以及K-means等聚类方法。监督的方法就可以使用熵来定义，先分成两部分，是的熵最小，取较大熵区间再分，循环往复。 不仅仅连续变量要离散化，离散的变量有时候还需要二元化。（为什么是二元化呢？因为常见的模型都是这样简化的。比如决策树）这时候就能体现出进制的伟大了！（6个人60桶酒问题）不过这种情况下常常会出现共线性的问题（类似dummy variable的情况）。或者就直接使用dummy variable。 根据自己经验的总结，感觉如果不是非常在乎度量的大小的话，尽量先标准化数据。（此条结果有待验证，有错误希望大家指出） 接下来再说说相似性、相异性的度量，为什么把这个也放在预处理这个阶段呢？因为许多算法例如聚类，KNN，异常检测等都需要相似相异的概念来衡量数据之间的亲疏远近，例如做系统聚类时，当计算出距离矩阵时就不需要原始数据了。如果不需要复杂的度量，可以参考下表： 属性类型 相异度 相似度 定类数据 dummy variable(相同取1，不同取0) 类似示性函数 定序 d=∥x−y∥n−1d=\\frac{\\| x-y \\|}{n-1}d=​n−1​​∥x−y∥​​ s=1−ds=1-ds=1−d 定距或定比 d=∥x−y∥d=\\| x-y \\| d=∥x−y∥ s=−d,s=11+d,s=e−d,s=1−d−min_dmax_d−min_ds=-d,s=\\frac{1}{1+d},s=e^{-d},s=1-\\frac{d-min\\_d}{max\\_d-min\\_d}s=−d,s=​1+d​​1​​,s=e​−d​​,s=1−​max_d−min_d​​d−min_d​​ 面对复杂的相异度我们经常使用距离这个概念，常用的距离有欧几里得距离，其实是明可夫斯基距离的一种特殊形式，也是L2范数。还有常用的就是L1范数，上确界距离。还有包括马氏距离也比较常用。 复杂的相似度度量。如果数据是离散的可以使用混淆矩阵。基于混淆矩阵的概念有简单匹配系数（SMC），Jaccard系数（面对非对称的数据），以及余弦相似度（对象规范化，减少计算时间，非对称数据常用，量值重要时不要用）。面对连续的数据，可以使用常用的皮尔逊相关系数，其中可以选中中位数、绝对差、trim均值作为改进方法。 距离的选择还可以使用加权的方法，有点类似核函数的意思。 3.探索数据 数据清洗好了，先看看数据大概是个什么样子吧。这个阶段常用的技术就是汇总（计算均值方差），可视化（画个矩阵散点图看看相关性）还有不怎么明白的OLAP。 数据汇总学过统计概率论的人都十分明白啦，就说受可视化吧。 低位数据的可视化就是统计中常用的一些方法，非常好用。包括，茎叶图、直方图、箱线图、饼图、散点图等。 高维数据的可视化就不能仅仅使用坐标这个概念了，可以加入颜色、大小、形状等等来展示不同的维度。还有非常嘻哈的Chernoff脸！ OLAP不怎么懂，不多说了，大家可以自行查找文献。 Copyright © Niu 2015 ~ 2018 all right reserved，powered by GitbookUpdate Time: 2018-05-23 17:18:59 "},"shu-ju-wa-jue-dao-lun/guan-lian-fen-xi.html":{"url":"shu-ju-wa-jue-dao-lun/guan-lian-fen-xi.html","title":"关联分析","keywords":"","body":"关联分析 1.概述 关联分析用于发现隐藏在大型数据集中的有意义的联系，所发现的联系可以用关联规则或频繁项集的形式表示。 进行关联分析的时候，需要处理两个关键问题： 从大型事务数据集中发现模式可能在计算上要付出很高的代价。 所发现的某些模式可能是虚假的，因为他们可能是偶然发生的。 接下来的讨论从上面两个问题展开，首先来说说第一个问题，解释关联分析相关的概念以及有效的挖掘这种模式的算法。 2.问题的定义 前面简单说明了关联分析的定义，下面展开说明一下什么是关联规则。 2.1 二元表示 关联分析面对的是什么样的数据呢？ 一个最最常见的例子就是啤酒尿布的故事，这也是关联规则的一个应用，面对这样的数据我们在「数据」一文中讲到是购物篮数据。购物篮数据有两个维度，一个是事务，一个是项集。购物篮数据类似列联表，分横纵两个维度，每一行代表一个事务，每一列代表一个项。那么购物篮里面的数据是什么样子呢？就是常见的二元变量（0，1变量），即该项是否出现在了该事务中，通常这样野蛮的表示一个事件是有问题的，就是非对称的问题，因为我们通常认为项在事务中出现比不出现更重要。同时，这样简单粗暴的用0、1表示，常常忽略数据某些重要方面，比如数量、价格等等。 2.2 支持度（support）&置信度（confidence） 先谈一个概念叫做支持度计数，即包含特定项集的事务个数。若以T=t1,t2,...,tNT={t_1,t_2,...,t_N}T=t​1​​,t​2​​,...,t​N​​表示所有事务的集合，以σ\\sigmaσ表示支持度计数，那么有以下关系： σ(X)=∥{ti∥X⊂ti,ti∈T}∥\\sigma(X) = \\|\\{t_i \\| X \\subset t_i, t_i \\in T \\}\\|σ(X)=∥{t​i​​∥X⊂t​i​​,t​i​​∈T}∥ 其中 ∥⋅∥\\|\\cdot\\|∥⋅∥ 表示集合中元素的个数。 那么什么是支持度和置信度呢？ 简单来说就是概率or频率（支持度）和条件概率（置信度）的概念。 支持度和置信度是用来描述关联规则的度量，关联规则是形如X→YX \\rightarrow YX→Y的表达式，X、Y都是某个项集，而且是不相交的项集。 支持度用来确定给定数据集的频繁程度，支持度确定Y在包含X的事务中出现的频繁程度。 s(X→Y)=σ(X∪Y)Ns(X \\rightarrow Y)=\\frac{\\sigma(X \\cup Y)}{N}s(X→Y)=​N​​σ(X∪Y)​​ c(X→Y)=σ(X∪Y)σ(X)c(X \\rightarrow Y)=\\frac{\\sigma(X \\cup Y)}{\\sigma(X)}c(X→Y)=​σ(X)​​σ(X∪Y)​​ 支持度用来保证规则的出现并不是偶然的，通常用来删除无意义的规则。置信度保证规则的可靠性。 Tips : 关联规则的推论并不必然蕴含因果关系，指标是规则前件与后件的项明显地同时出现。因果关系需要利用常识或其他知识补充来判断。 2.3 定义关联规则 关联规则怎么做呢？就一句话，关联规则就是找出支持度大于等于 minsup 并且置信度大于等于 minconf 的所有规则。其中 minsup 和 minconf 是对应支持度和置信度阈值。 是不是很简单，但是具体怎么做呢？要找到每一条规则然后分别计算置信度和支持度吗？ 这确实是最简单粗暴的方法，但是其计算代价巨大。对于一个包含ddd个项的数据集中提取的可能规则的总数为： R=3d−2d+1+1R = 3^d - 2^{d+1} +1R=3​d​​−2​d+1​​+1 为了避免不必要的计算，可以事先对规则进行剪枝。 首先我们将关联规则的算法分成两步，首先计算其支持度然后再计算其置信度。认真点来说就是以下两步： 频繁项集产生。目标是发现所有满足最小支持度的项集，这些项集成为频繁项集。 规则的产生。 目标是从上一步发现的频繁项集中提取满足最小置信度的所有规则，这些规则称为强规则。 通常来说，频繁项集产生的计算开销远大于生产规则所需的计算开销。接下来将详细描述这两个关联分析算法中的关键步骤。 3.频繁项集的产生 假如我们执意使用枚举的方法产生频繁项集，那让我们来算一算。 对于包含k个项的数据集可能产2k−12^k - 12​k​​−1个频繁项集，不包含空集在内（C_k1+C_k2+C_k3+...+C_kkC\\_{k}^{1}+C\\_k^2+C\\_k^3+...+C\\_k^kC_k​1​​+C_k​2​​+C_k​3​​+...+C_k​k​​)。而随着k的增大，这个计算量可以说是指数级的增长，需要搜索的空间也随之指数级的增长。那么我们来看看计算量具体是多少， 假如对于上述的k个项的数据集而言一共有N个事务与之对应，最大事务宽度是w，那么通过比较每个事务与每个可能项集的时间复杂度是O(NMw)，其中M=2k−1M=2^k-1M=2​k​​−1是候选集数。 上面的时间复杂度中w是无法改变的事实，那么我们通过优化N、M来降低频繁项集产生的计算复杂度。 对于M，即减少候选项集的数目而言，我们采用先验原理（apriori）。对于减少比较次数而言，我们采用更高级的数据结构或者存储候选项集或者压缩数据集，接下来我们将详细说明这两个方面。 3.1 先验原理 先验原理: 如果一个项集是频繁的，则它的所有子集也一定是频繁的。 这个原理非常好理解，如果一个项集是频繁的，那么它的子集的支持度一定是大于它自己的，因为子集被包含的事务个数一定是更多的，所以如果一个项集是频繁的，那么它的所有子集也一定是频繁的。这个性质也称为支持度度量的反单调性。同样的它的逆否定理同样成立。 先验原理的逆否定理：如果一个项集是非频繁的，那么它的所有超集也一定是非频繁的。 这种基于支持度度量修剪指数搜索空间的策略成为基于支持度的剪枝。 3.2 Apriori算法的频繁项集产生 Apriori算法使用了基于支持度的剪枝方法来控制候选集的指数增长。具体步骤如下： 扫描所有数据，得出所有单项集的支持度。 将上一步中所有满足最小支持度的频繁项集进行组合得到2-项集。 循环第二步直到没有新的频繁项集产生。 该算法有两个特点：第一，它是一个逐层算法，从频繁1-项集到最长的频繁项集，需要遍历所有长度项集的每一层；第二，它使用产生——测试的策略来发现频繁项集，每次迭代都通过前一次迭代产生的频繁项集产生，然后计算这次迭代所有项集的支持度并与minsup比较。 下面就算法中的具体细节展开说明。 3.2.1 候选的产生与剪枝 上面一小节说到Apriori算法中第二步是产生后算计并且对不满足最小支持度的项集进行剪枝，在这个过程中需要注意的是产生候选集的要求： 避免产生太多不必要的候选集。候选集必须保证其所有真子集（这里我感觉书中的意思不是全部的真子集，而是全部的k-1项集，k是当前项集的宽度，因为关联规则的方法是逐层算法，只需要保证k-1阶即可保证全局最优解？）都是非频繁的，这样可以保证该候选集是频繁项集。 保证候选集的集合是完全的，即必须包含所有应该出现的频繁项集。 避免产生重复的候选集。例如一个4-项集{a,b,c,d}的产生可能有多种：{a}和{b,c,d}；{b,c}和{a,d}等等。 为了满足上述要求我们简述集中产生候选集的方法： 蛮力方法。 蛮力方法将所有的k-项集都看作候选集，然后再使用剪枝的方法去除不必要的项集。乍一看确实非常简单，但是计算开销巨大，下面来看看其时间复杂度。 假设一共有d个项，那么使用蛮力方法可以产生C_dkC\\_d^kC_d​k​​个k-项集集，每一个候选集可能产生k个k-1项集，然后每个k-1项集需要d个项比较来确定是否其真子集都是频繁项集。总结来说，这种方法的时间复杂度是O(Σ_k=1dkC_dk)=O(d⋅2d−1)O(\\Sigma\\_{k=1}^dkC\\_d^k)=O(d\\cdot2^{d-1})O(Σ_k=1​d​​kC_d​k​​)=O(d⋅2​d−1​​)。 可以看出这是一种指数型复杂度，非常\"蛮力\"！ F_k−1×F_1F\\_{k-1} \\times F\\_1F_k−1×F_1 方法。 这种产生候选集的方法是利用频繁1-项集来扩展每个(k-1)-项集。这种方法将产生O(\\|F_{k-1}\\|\\|F_1\\|)个候选k-项集。这种方法的复杂是O(Σk∥Fk−1∥∥F1∥)O(\\Sigma_k\\|F_{k-1}\\|\\|F_1\\|)O(Σ​k​​∥F​k−1​​∥∥F​1​​∥)。 相较于蛮力方法复杂度降低了不少，但是同样的非常容易产生重复候选项集。解决重复的方法可以利用字典编号的方法，将所有项进行编号，在进行k-项候选集的产生时，只选取比k-1项集中所有项编号都大的项进行组合，这样就可以避免重复问题。 该方法另外一个缺点是，仍然会产生不必要的项集，例如将频繁2-项集{a,b}和c结合得到3-项集{a,b,c}，但是其真子集{a,c}却不是频繁2-项集，这样就会导致该3-项集也是非频繁项集。这样的情况下，我们的解决方法是：对于每一个幸免于剪枝的候选k-项集来说，它的每一个项必须至少在k-1个(k-1)-项集中出现，否则，该候选集就是非频繁的。解释一下就是，k-项集的所有(k-1)-项集子集必须出现在k-1频繁项集中，k项集中的每个元素起码出现两次才能保证其k-1项集有可能是频繁的，假如某个项只出现了一次，那么其必定有一个k-1项集的组合是非频繁的。这个方法可以减少非频繁项集的产生，但是不能保证所有通过此方法得到的候选项集一定是频繁的。 Fk−1×Fk−1F_{k-1}\\times F_{k-1}F​k−1​​×F​k−1​​方法。 这种候k项候选集产生的方法是合并一对频繁(k-1)-项集，仅当它们的前 k-2 个项都相同的时候。 使用这种方法的一个先决条件是，候选项集产生的过程中必须使用了字典序方法进行组合。 为什么一定要前 k-2 项都相同呢，和 Fk−1×F1F_{k-1} \\times F_1F​k−1​​×F​1​​ 方法中剪枝的思想类似，我们必须要求产生的k项集的所有k-1项集都是频繁的，那么当前k-2项都相同的时候，必然能保证其所有k-1项子集都是频繁的。可以想象一个产生3-项集的例子就很容易理解了。 3.2.2 支持度的计数 上一步产生了候选项集，那么如何筛选候选项集成为频繁项集呢？就需要计算其支持度与minsup的比较。 最容易想到的支持度计数方法就是通过比较每个事务与所有的候选集，并且相应的更新包含在事务中的候选项集的支持度计数。这种方法的计算方法是昂贵的。 不妨换个思路，我们先枚举出所有事务包含的项集，并且利用它们更新对应的候选项集的支持度。因为总体项集的数目比事物所包含的项集数目会大很多，所以这种方法会比之前的计算量上小很多。那么还有更好的方法吗？ 还很有，听说过HASH吗？如果是分布式的工具用得比较多的话，这个概念应该接触的还挺多的，将所有的事物以及项集全部加载入一个HASH搜索树中，这样的话，每次比较只需要比较每个HASH分支下的项集与事物即可，而不必将整个项集与事物全部组合计算支持度，yep，确实不错。 3.3 计算的复杂度 总结以上方法，发现频繁项集的产生的计算复杂度主要受一下几个方面影响： 支持度的阈值：降低支持度的阈值通常会导致更多的频繁项集 项的个数：项的个数的增多将导致计算及IO开销的增大 事务的个数：由于Apriori算法会反复扫描数据集，因此事务的个数增加会导致运行时间的增加 事务的平均宽度：事务的平均宽度将导致产生候选项集及支持度计算时考虑跟多的候选项集 4.规则的产生 如果湖绿前件活后件为空的规则化，一个包含k个项所产生的规则的个数为 2k−22^k - 22​k​​−2，规则产生的可以简单的理解为将某个项集Y划分为两个不为空的子集，如果这两个子集所产生的规则曼度置信度的阈值那么一个规则就由此产生了。 4.1 基于置信度的剪枝 非常遗憾的是置信度不像支持度具有单调性的性质，但是，还是有希望的，如下定理可以帮助我们： 如果规则 X→Y−XX \\rightarrow Y-XX→Y−X不满足置信度阈值，那么形如 X′→Y−X′ X^{'} \\rightarrow Y-X^{'}X​​′​​​​→Y−X​​′​​​​的规则也一定不满足置信度的阈值，其中X′X^{'}X​​′​​​​是XXX的子集。 如何解释这个定理呢？首先列出置信度的概念，规则X→Y−XX \\rightarrow Y-XX→Y−X的置信度为σ(Y)/σ(X)\\sigma(Y)/\\sigma(X)σ(Y)/σ(X)，而X′→Y−X′ X^{'} \\rightarrow Y-X^{'}X​​′​​​​→Y−X​​′​​​​的置信度为σ(Y)/σ(X′)\\sigma(Y)/\\sigma(X^{'})σ(Y)/σ(X​​′​​​​)，注意σ()\\sigma()σ()的定义为该项集所被包含的事务的个数，因此σ(X′)>=σ(X)\\sigma(X^{'})>= \\sigma(X)σ(X​​′​​​​)>=σ(X)，因此X→Y−XX \\rightarrow Y-XX→Y−X的置信度一定是大于X′→Y−X′ X^{'} \\rightarrow Y-X^{'}X​​′​​​​→Y−X​​′​​​​的。 4.2 Apriori算法中规则的产生 Apriori算法通过一种逐层的方法来产生候选规则，每层对应于规则中后件中的项数。一开始，提取的规则的后件中只包含一个项，通过将所有规则与阈值比较，过滤规则，然后再使用这些规则来产生新的候选规则。如果某个结点具有低置信度，根据4.1所阐述的定理进行剪枝。举例说明如果abc→d{abc} \\rightarrow {d}abc→d具有低置信度，那么ab→cd{ab} \\rightarrow {cd}ab→cd、ac→bd{ac} \\rightarrow {bd}ac→bd、bc→ad{bc} \\rightarrow {ad}bc→ad、a→bcd{a} \\rightarrow {bcd}a→bcd、b→acd{b} \\rightarrow {acd}b→acd、c→abd{c} \\rightarrow {abd}c→abd就可以直接剪掉。 Copyright © Niu 2015 ~ 2018 all right reserved，powered by GitbookUpdate Time: 2018-05-22 20:50:34 "},"shu-ju-wa-jue-dao-lun/ju-lei-fen-xi.html":{"url":"shu-ju-wa-jue-dao-lun/ju-lei-fen-xi.html","title":"聚类分析","keywords":"","body":"1.聚类概述 聚类是什么做什么的呢？ 简单通俗的说就是将数据分成有意义或有用的组（簇），能够反映出数据的自然结构。聚类很多情况下都是其他问题的一个起点、或者说是一种准备工作。生物学中的界门纲目科属种就是一种层次化聚类，将杂乱无章的客户信息分成不同的组以实施不同的促销手段都是聚类的应用。聚类的作用大体来说分为： 汇总。常见的汇总比方说PCA、SVD，但是面对数据特别复杂的时候是无法发挥作用的，所以需要简便的工具，例如聚类方法。 压缩。这个技术常常用于图像、声音、视频的处理，他们数据特点都是：数据之间相似度较高；某些信息的丢失是可以接受的；希望发幅度压缩数据。 有效发现最邻近。先找到邻近的簇，然后再从邻近的簇中选择邻近的点。 前面通俗的说了聚类的定义，具体来说就是仅根据在数据中发现的描述对象以及其关系信息，将数据对象分组。目标就是将使得组内的相似性很大，而组间的相似性很小。 聚类的类型是多种多样的，有一些定义需要区分： 层次的与划分的。 划分聚类，即簇的集合是非嵌套的，每个数据恰好在一个组中，例如K-means。层次聚类的簇集合是嵌套的，组成一个树（或者叫谱系图？），层次聚类可以看做划分聚类的序列，划分聚类可以通过取特定层得到。 互斥的、重叠与模糊的。 互斥，即每个对象只能指派到一个簇中。而重叠和模糊相反。重叠的现实意义是存在的，例如一个人可能是老师也可能是学生，因此需要将他指派到两个簇中。而模糊就是数学上模糊集的概念，每个数据对象不是一定属于或者不属于某个簇，而是以一定的概率分配给每个簇，但是实践中通过将对象指派到概率或者说隶属权值最高的簇中，因此模糊聚类也就转化成了互斥聚类。 完全的与部分的。 完全聚类，是指每个对象都能指派到一个簇中，而部分聚类相反。部分聚类的存在是因为为了舍弃一些噪声点、离群点或不感兴趣的点。 同时簇的类型也是多样的，不同的聚类方式使用的簇的类型也是不同的。粗略来说簇的类型大致可以分为：明显分离的，基于原型的，基于图的（连通分支、基于邻近的簇），基于密度的，共同性质的（概念簇）。 2.K-means Okay，废话了这么多，我们就来说说具体的方法吧，先从最熟悉的K-means说起。 K均值算法可以大概描述为以下几个步骤： 选择K个初始质心，K是需要人工选择的一个参数，就是所期望的簇的个数。 每个点指派到最近的质心，指派到同一质心的点形成同一个簇。 对上一步形成的每个簇计算新的质心。 重复第2、第3步，直到簇或者质心不发生变化。 K均值总是会收敛到一个解，这是不同担心的，而且收敛通常发成在早期阶段，所以放宽一下第4步的要求，例如只有百分之一的点发成变化即可。 下面我们将每一步拆开详细的来说。 2.1 选择初始质心 K均值最大的一个问题，就是初始质心的选择会导致不一样的结果，而且某些结果效果可能会非常差。常见的方法就是随机的选择质心，这种方法就不是很好的一个选择。通常的处理方法也很简单：多运行几次，每次随机选取不同的初始质心，然后最后选择具有最小SSE的簇。这个方法虽然简单，但是效果可能一般，取决于我们的数据集以及簇的个数。 那么我们可以通过使用层次化聚类对他进行改进，首先对样本进行层次化聚类，然后提取K个簇，并将这些簇的质心最为初始质心。这种方法通常很有效，可是感觉很罗嗦有没有？他只能应用于样本较小，而且K相对于样本大小较小的情况。层次化聚类已经很繁琐了，而且层次化聚类已有的结果拿来再聚类，感觉不怎么实用。 另一种方式是首先随机的选取一个点（或者直接选择样本的均值作为质心），然后对每个后续的初始质心，选择离选取过的质心最远的点。这样就确保了我们的初始质心是散开的，但是有一个问题是这样很可能选取到离群点，而且计算距离最远点的开销也很大。 还有一种处理方式是K-means的改进，二分K-means方法。 还还有一种方法是使用后处理来修补所产生的簇集。 这两种方法放到后面介绍。 2.2 指派点到最近的质心 对于欧式空间中的点，我们通常使用欧几里得距离（L2L_2L​2​​）计算，对于文档通常使用余弦相似性计算，通常还有例如曼哈顿距离（L1L_1L​1​​）和Jaccard度量。 K均值的相似性度量需要比较简单的相似性度量，因为算法每一步都要重复的计算每个点的与质心的相似度。还有例如二分K均值通过减少相似度计算量来加快K均值速度。 2.3目标函数以及更新质心 聚类的目标通常用一个目标函数表示，该函数依赖于点之间，或者点到质心的邻近性。对于欧几里得空间中的数据，我们常常使用误差平方和（SSE）来作为度量聚类质量的目标函数。SSE也成为散布。使得SSE最小的质心是均值。对于文档数据通常使用余弦相似度，也称作簇的凝聚度。其最优解也是均值。而曼哈顿距离作为度量的话，最优质心是中位数。而上面说到的欧几里得距离以及余弦相似度都可以看作是Bregman散度的特例，Bregman散度的最优解都是均值。 2.4其他问题 空簇问题 K均值存在的问题之一就是空簇的问题，就是说可能所有点都没有指派到某个簇内。这种情况下，通常会选择一个替补质心，否则平方误差会加大。一种方法是选择一个距离当前任何质心最远的点，或者从具有最大SSE簇中选择一个替补质心。 离群点 使用SSE标准的时候，离群点可能会过度影响所发现的簇，所得到的质心可能没有那么有代表性，提前删除他们是比较有用的方法。当然某些情况下是不能删除离群点的，因为有些时候离群点可能是人们感兴趣的点。 那么如何识别离群点呢？提前删除是非常好的结果，如何提前检测在后来的章节会说到。还有一种方法是使用后处理的方法，例如可以记录每个点对SSE的影响，删除那些具有异乎寻常的影响点，此外还可以删除那些很小的簇，因为他们常常代表离群点。 后处理方法 一种明显降低SSE的方法是使用较大的K，然而很多情况下我们只想降低SSE，而不希望增加簇的个数，怎么办呢？一种常用的方法是交替的使用簇分裂和簇合并。 增加簇个数的方法包括（分裂）：分裂一个具有较大SSE的簇或者分裂在特定属性上具有最大标准差的簇；引进一个新的质点，通常选择离所有簇质心最远的点，另一种方法是从所有的点或者具有最高SSE的点中随机选择。 减少簇个数的方法包括（合并）：拆散一个簇，删除簇对应的质心，簇中的点重新指派到其他簇中；合并两个簇，通常选择质心距离最近的两个簇（质心法），或者选择合并后导致总SSE增加最少的簇（Ward法）。 增量地更新质心 可以在点到簇的每次指派之后，增量的更新质心，而不是在所有点都指派到簇中之后再更新。这样还可以保证不会产生空簇（因为所有的簇都是从单个点开始）。此外，如果使用增量更新还可以调整点的相对权值，增加准确率和收敛的速度，但是这样做通常相当困难（类似神经网络训练的权值）。缺点方面是，增量地更新质心肯能导致次序的依赖性，而且计算量更加复杂。 2.5 二分K均值 二分K均值的思想非常简单，为了得到K的簇，首先将所有的簇分成两个簇，然后从中选取一个簇进行二分裂，然后在从所有的簇中选择一个进行二分裂，知道生成K个簇。 待分裂簇的选择有许多种方法，一个是选择最大的簇，或者选择具有最大SSE的簇，或者选择一个基于大小和SSE的综合指标进行选择。K均值可以得到使总SSE局部最小的聚类，而二分K均值是不能保证的，因为它只是局部的使用了K均值算法。 前面也提到过了，二分K均值的有点就是不会受初始值选定的影响，而且速度更快。而且我们通常是使用已经聚类好的结果簇作为基本K均值的质心，对结果进行逐步求精。 2.6 K均值的适用簇类型 前面说过簇的类型分为很多种，K均值对于非球形形状或具有不同尺寸或密度的簇时，很难检测到“自然的”簇。K均值的目标函数是最小化等尺寸和等密度的球形簇，或者明显分离的簇，但是如果可以接受将一个自然簇分割成若干个子簇的话，这些局限性可以在某种意义上克服。 2.7 K-means优缺点 K均值简单而且可以用于各种类型的数据，也相当有效（毕竟是最古老最广泛使用的聚类方法），而且K均值的某些变种甚至更加有效。K均值不适合处理那些非球形簇、不同尺寸、不同密度的簇，而且对包含离群点的数据进行聚类是，K均值也是有问题的。 3.凝聚层次聚类 多元学过系统聚类，这就是我们所说的凝聚层次聚类。 层次聚类通常分为两种：凝聚的，从点作为个体簇开始，每一步合并两个最接近的簇，直到所有点合并成为一个簇；分裂的，从包含所有点的簇开始，每一步分裂一个簇，直到仅剩下单点簇。 这里先说凝聚层次聚类。凝聚层次聚类通常使用树状图（谱系图）显示，对于二维点的集合也可以使用嵌套图表示。 3.1 基本凝聚层次聚类算法 从个体点作为簇开始，一次合并两个最接近的簇，直到最后只剩下一个簇。 那么这里的关键就是如何定义两个簇之间的邻近度。系统聚类里面说了8种方法，这里简单的说一下其中三个最简单的： 最长距离法（MAX，全链）利用两个簇中两个最远的点之间的邻近度作为簇的邻近度，即不同结点子集中两个结点之间的最长边。完全链接对于噪声点和离群点不太敏感，但是可能会使大的簇破裂，并且偏好球形簇(无法理解后两条，有高见的同学请帮理解一下)。 最短距离法（MIN，单链）定义簇的邻近度为两个簇中距离最近的两个点之前的邻近度。单链技术擅长处理非椭圆形的簇，对噪声和离群点很敏感。 平均距离法（组平均，group average）定义簇的邻近度为不同簇所有点对邻近度的平均值。 此外，还有两种方法，是基于原型的观点，簇用质心代表，使用质心之间的邻近度作为簇的邻近度。这个叫做质心法，质心法看上去K均值相似，其实更相似的是下面的Ward法，质心法的一个特点是：倒置的可能性，即被合并的簇可能比前一步合并的簇更加相似，对于其他方法被合并的簇之间的邻近度随着层次聚类的进展单调的增加（或者不增加）。因此这种特性通常被视作一种缺点。 另一种基于原型的方法是Ward方法，使用合并两个簇导致的SSE增加来度量两个簇之间的邻近度。该方法与K均值使用的目标函数相同，而且如果两个点之间的邻近度使用它们之间距离的平方计算时，Ward方法与组平均法十分相似。 3.2 优缺点 层次聚类可以产生层次结构，这可能正是某些数据所需要的结构，而且某些研究表明（...书中原话，感觉十分不靠谱），层次聚类可以产生高质量的聚类。 缺点来来来，一个个列出来： 缺乏全局目标函数。层次聚类在每一步局部的使用一些标准来决定哪些簇应当合并，不能全局优化。但是这样做可以劈开解决困难的组合优化问题，而且解决了局部最小问题以及初始点的选择问题。 合并是最终的。一旦两个簇进行合并，就不能撤消了。这种形式阻碍了局部最优变成全局最优。尽管如此，很多时候Ward还是作为一种初始化K均值聚类的鲁棒方法使用。解决这个问题的方法是：移动树的分支以改善全局目标函数，或者使用划分聚类技术（K均值）来创建许多小簇，然后从这些小簇出发进行层次聚类。 计算量与存储量的代价是昂贵的。层次化聚类需要计算距离矩阵（期末考试手动计算...简直惨无人道），而且还要存储这个距离矩阵，所以面对数据量较大的情况，层次化聚类通常是乏力的。 对于噪声、高维数据的处理能力差。因为所有合并都是最终的。 对于2、3、4问题都可以使用K均值进行部分聚类，在某种程度上可以解决这三个问题。 4.DBSCAN Copyright © Niu 2015 ~ 2018 all right reserved，powered by GitbookUpdate Time: 2018-05-22 20:50:34 "},"shu-ju-wa-jue-dao-lun/fen-lei-1-0.html":{"url":"shu-ju-wa-jue-dao-lun/fen-lei-1-0.html","title":"分类1.0","keywords":"","body":"1.决策树 OK，菜都洗好了，就准备一下做什么菜吧。菜谱可是很重要的。从这开始就来讲讲各种常见的算法吧。先从决策树开始讲起。 决策树是在做什么呢，就是分类，分类又可以干什么呢？看了第一节的话，大家应该知道就是预测和描述两个功能。 解决分类问题的一般方法就是给定一个训练集，来训练一个分类模型，然后再用检验集来评估其性能，可以用混淆矩阵的概念来做。 决策树的概念非常形象，就像一颗倒置的树，从根节点出发，到达不同的内部节点，最后终结在叶节点。 如何构建一个决策树呢？大致的思想可以一句话说明，选取一个属性测试条件，将记录划分成越来越小，越来越纯的子集，使得最后的子集中的所有记录属于同一个类。当然这是最理想的情况，有可能无法选出一个属性测试条件，也有可能不能继续划分子集。 解决决策构建的两个重要问题就是如何分裂训练记录，以及如何停止分裂过程。 首先看如何分裂训练记录，这就要先知道都什么什么样的“分裂”。有的属性只有两个可能的结果，有的属性可能有多个，有的属性是序数属性，其结果不能随意分割，连续属性的分裂方法那就太多了。 那么如何选择最佳的分裂方式呢？ 要知道分裂就是为了让子集更纯，如何定义纯呢？我们用熵来定义混乱即可，此外还可以供Gini指数、错误率、信息增益来作为混乱度的评价。但是这样定义的不纯度的度量更有利于具有大量不同值的属性，所以可以采用增益率来作为划分标准。增益率如下定义： Gainratio=ΔinfoSplit Info Gain ratio = \\frac{\\Delta_{info}}{Split\\ Info} Gainratio=​Split Info​​Δ​info​​​​ 其中Δinfo\\Delta_{info}Δ​info​​为信息增益，Split Info=−Σi=1kP(vi)log2P(Vi)Split\\ Info=-\\Sigma^k_{i=1}P(v_i)log_2P(V_i)Split Info=−Σ​i=1​k​​P(v​i​​)log​2​​P(V​i​​)，k是划分的总数。 决策树划分好了，但是很有可能出现了过度拟合的情况。什么情况会导致过分拟合呢？首先是噪声，第二是样本量过小，第三是维度过多（多重选择问题，50个人预测至少预测对10只股票中的8只得概率是0.9399）。 如何处理过度拟合的问题呢？两种方法，先剪枝，当不纯度量增量小于某个阈值的时候就停止分裂。缺点是，阈值很难选，而且即使当前的增益率很低，接下来的字数的增益率有可能很高，剪掉了较好的子树。还有一种方法是后剪枝，后剪枝也有两种常用的方法，用新的叶节点替换子树（子树替换），或用子树中最常见的分支代替子树（子树提升）。 那么一个决策树出现了，如何评价他的性能呢？ Holdout 简单来说就是我们最常用的划分训练集和检验集的方法。局限性很多： a.用于训练样本的数据变小。 b.模型可能高度依赖于训练集与检验集的构成。 c.训练集与检验集之间不独立。 随机二次抽样 多次重复的Holdout方法，将准确率取均值即可。 Cross-Validation交叉验证。非常常用的方法，将训练集等分成几分，选取其中的一份作为检验集剩下的作为训练集，重复以上步骤知道所有集合都被选作过检验集，然后吧结果平均即可。 Boostiong。神一样的Boosting。训练记录采用又放回抽样，没有抽中的记录就成为检验集的一部分。每次抽样计算一次准确率。如何计算模型的准确率呢？这里就不是平均了，是.632 bootstrap（0.632是样本量足够大时，大小为N的自助样本包含原始数据中约63.2%的数据）。它通过组合每个自助样本的准确率和由包含所有标记样本的训练集计算的准确率来构成。 {% math %} acc{boot}=\\frac{1}{b}\\Sigma^b{i=1}(0.632 \\times \\epsilon_i + 0.368 \\times acc_s){% endmath %} 如何比较两个分类器呢？ 记住假设检验的威力。两个模型是否有显著差别得看假设检验。决策树准确率是一个二项分布，样本量足够大的时候可以近似为正态，所以假设检验的置信区间可以正态分布来做。比较不同分类方法也是如此。 说了这么多，我们来看看决策树有什么特点吧。 决策树不需要对数据进行任何假设，因为它是一种非参数的方法。 决策树非常简单，易于构建，解释性也比较强。 决策树在面对缺失值以及高维问题时有很好的鲁棒性。 决策树的决策边界全部是平行于坐标轴的，对于某些数据（例如用x+y=1完全分割的数据）的分类能力很低（但是可以利用斜决策树解决，即其测试条件变为一条斜线例如：x+y=1，但是如果是这样还不如直接用线性回归） 不纯度的度量的选择对决策树的影响很小，剪枝对决策树的影响可能会更大。所以如果效果不好、不满意一定要剪枝呀。 找到最佳决策树是NP完全问题。 2.分类的其他技术 上一篇讲到了决策树的构建、修剪、性能评价等方面的知识，然而分类方法不仅仅只有决策树，还有许多种方法，这一节主要讲讲其他几个常见的方法。 2.1基于规则的分类器 什么是基于规则的分类器呢？简单来说就是一个if...then...的过程，符合一定的规则就将其分到符合这个规则的类中。规则的集合就称作规则集，规则集中的每一条小规则的前提成为规则前件，规则前件有许多个合取项构成，规则的预测类成为规则后件。 分类规则的优劣由两个条件左右，一个是覆盖率，一个是准确率。覆盖率是数据集中出发该规则记录所占的比例，准确率是出发该规则记录中类标号正确的比例。 如何构建一个规则分类器呢？规则集的产生需要满足两个条件：互斥规则和穷举规则。互斥即同一个记录不能同时被两个或以上的规则出发，穷举即属性集的任一组合都被规则集中的规则覆盖。如果这两个规则同时满足，就保证了每一条记录只能被有且只有一个规则触发。 然而现实中这种情况有可能是无法实现的，如果穷尽原则没有满足，我们需要建立一个前件为空的规则作为默认规则来覆盖，这个默认规则指向一个默认类。如果规则不是互斥的，规则的预测可能会相互冲突，有一下两种方法来解决：有序规则和无序规则。有序规则，将规则进行排序，若同时满足一个，则选取排序最高的规则触发。无序规则将同一记录的触发的规则指向的类进行统计，然后采用投票最多的类。无序规则的优势在于其对于噪声或者错误的鲁棒性很强，而有序的非常敏感。而且其建立的模型开销很小，不需要去维护规则的顺序。但是它的计算量很大，每一条记录都要同每一条规则进行匹配。 有序规则的构建有两种常见的方案：基于规则的排序方案和基于类的排序方案。基于规则的排序方案假设该方案之前的规则全部是不成立的，当规则数量很大的时候，在尾部的规则将很难解释。基于类的排序方案是把同一类的规则在规则集中同时出现，同一个类之间规则的相对顺序不重要，主要其中一个被触发就赋予该规则的分类。 说了这么多，那么规则到底是如何提取出来的呢？这里只说明基于类的规则排序方法的提取方法。 首先是直接方法，这里只介绍一下广泛使用的RIPPER算法。对于多类问题，先按类的频率进行排序，假设(y1,y2,...,yc)(y_1,y_2,...,y_c)(y​1​​,y​2​​,...,y​c​​)是排序后的类，其中y1y_1y​1​​是最不频繁的类，在第一次迭代中，将属于y1y_1y​1​​的样例标记为正例，而把其他类的样例标记为反例，使用顺序覆盖算法产生区分正例和反例的规则（顺序覆盖算法即先找出能够覆盖最多样例的规则，然后将这个规则覆盖的样例删掉，选取剩下集合中覆盖样例最多的规则，如此循环）。接下来提取区分y2y_2y​2​​和其他类的规则。重复该过程，直到剩下类ycy_cy​c​​，此时将ycy_cy​c​​作为默认类。 直接方法过后一定是一个间接方法。间接方法由决策树等分类器构建，然后将其中一些规则简化即可。 那么大家肯定要问了，既然如此不如直接用决策树好了。基于规则的分类器有什么好处呢？就是更容易解释模型。基于规则的分类器实际上相比决策树更加复杂，因为他可以同时出发多个条件，可以构造更加复杂的决策边界。基于规则的分类器，尤其是采用基于类的规则定序方法非常适合处理不平衡的数据集。 2.2最近邻分类器 大名鼎鼎的KNN，在ESL的第二章就已经开始介绍了，是一种简便而又实用的算法。将KNN之前不妨看看这句谚语。 If it looks like a duck, quacks like a duck and walks like a duck, it‘s a duck. KNN就是这么一种思想，如果某个目标值周围的点都是属于某一个类，那么这个点也是属于这个类的。 KNN算法的关键在于邻近点个数kkk的选取，太小了容易受噪声影响，过度拟合，过大的话可能会导致错误分类，而且最邻近也变得不那么“邻近”了。 不多说KNN了，大有兴趣可以去看[ESL第二章]的讲解 2.3贝叶斯分类器 说到Bayes，这个都不需要用大名鼎鼎来绍了，统计有一个分支就是贝叶斯统计。贝叶斯公式想必大家都知道了，它为什么这么牛呢，因为它把先验知识和从数据中提取出的信息相结合起来了！大家的先验知识就是我们所说的先验概率，而结合了我们数据中的信息所得到的概率（一般是个条件概率）就是后验概率啦。 那么我们接下来就介绍两种分类器。 2.3.1朴素贝叶斯分类器 英文叫做Naive Bayes，钟老师说过这个Naive就是说这个分类器太天真了，不过确实也很好用（没有对的模型，只有有用的模型）。为什么说它天真呢，因为他的假设条件一看就不可能成立，它假设每个属性（每个变量）之间条件独立。什么是条件独立？ P(X∣Y,Z)=P(X∣Z) P(X|Y,Z)=P(X|Z)P(X∣Y,Z)=P(X∣Z) 以上公式成立即说明X,Z条件独立于Y，上面的公式也可以写成： P(X,Y∣Z)=P(X∣Z)×P(Y∣Z)P(X,Y|Z)=P(X|Z) \\times P(Y|Z)P(X,Y∣Z)=P(X∣Z)×P(Y∣Z) 朴素贝叶斯的假设条件就可以表示为如下： P(X∣Y=y)=∏i=1dP(Xi∣Y=y)P(X|Y=y)=\\prod\\limits_{i=1}^dP(X_i|Y=y)P(X∣Y=y)=​i=1​∏​d​​P(X​i​​∣Y=y) 其中X={X1,X2,...,Xd}X=\\left\\{ X_1,X_2,...,X_d\\right\\}X={X​1​​,X​2​​,...,X​d​​}包含d个属性。 那么每个后验概率就是： P(Y∣X)=P(Y)∏i=1dP(Xi∣Y)P(X)P(Y|X)=\\frac{P(Y)\\prod_{i=1}^dP(X_i|Y)}{P(X)}P(Y∣X)=​P(X)​​P(Y)∏​i=1​d​​P(X​i​​∣Y)​​ [ESL第二章]讲到贝叶斯方法通过最小化Loss Function之后，得到结果就是选取某个最大化后验概率条件的类。 所以我们的目标就是最大化上面这个后验概率。如何估计这个后验概率呢？如果目标是离散的情况，可以直接用频率来代替。那么如果是连续变量的话，第一种方法就是预处理中提到的将连续变量离散化，第二种方法就是假定变量服从某个分布，例如正态，然后用样本均值和方差替代正态分布中的参数，再计算出每个样本点对应的概率。如果大家基础比较好，可能会说连续变量的概率密度中某一点的概率为0啊，其实这里是用了一个近似，假设在目标点周围一个足够小的区间内计算概率，近似的用目标点的值带入即可（page 143）。 如果大家实际计算过贝叶斯的问题会发现，这个连乘的概率非常脆弱，若果某一项为0，不管其他项多大，结果都是0，而某个样本出现频率为0不一定代表他真的没有出现，只是没有被观测到。所以如何解决这个问题呢？一种方法是m估计，强行的给他们一个小概率。还有一个方法是Good-Turing Estimate（古德-图灵估计，可以参考「数学之美」page 35），看到过，不知道是否可行，有待验证。 评价Bayes分类器的一个标准就是Bayes error rate，即贝叶斯误差率，是被错误分到别的类下面的样本点的比率（离散情况很好理解，那么连续的呢？）。 朴素Bayes分类器有如下特点： 1.面对孤立的噪声点和无关属性，Bayes Classifier is robust，噪声点的影响会被平均，而无关变量基本是一个均匀分布，不会影响后验概率的比较。 2.相关属性会降低Bayes分类器的性能，因为不符合假设呀！ 2.3.2贝叶斯信念网络（Bayesian belief networks,BBN） 为什么不用Naive了呢？就是因为它太Naive了，天真的让人不敢相信。所以就出现了贝叶斯信念网络，他不要求所有的属性条件都独立，而是允许指定哪些属性条件独立。 贝叶斯信念网络有两个主要的组成部分：一个有向无环图（表示各变量之间的依赖性），一个概率表（将各个结点与其父结点关联起来）。 贝叶斯网络一个重要性质是：贝叶斯网络中的一个结点，如果它的父结点已知，则它条件独立于它的所有非后代结点。 概率表的算法是，如果该点没有父母结点，则表中只包含其先验概率。若含有父结点，则该表包含条件概率 P(X∥Y1,Y2,...,Yk)P(X\\|Y_1,Y_2,...,Y_k)P(X∥Y​1​​,Y​2​​,...,Y​k​​) 建立一个BNN首先需要用“专家”知识对所有的属性进行排序，然后从头开始，每个属性将其前面的所有属性作为条件，然后剔除无关的条件，根据最后的条件概率画出有向图。 可以看出BNN的先决条件是属性的排序，不同的排序将会影响结果，所以这也是BNN的缺陷之一，如何解决呢？就是尽量将属性分为原因与结果，这样就不会有太多的排序可能出现，这就需要“专家”的力量啦！ BNN有什么特点呢？ 首先可以看出网络的构建是十分复杂的，但是一旦构建起来添加新变量也非常容易。适合处理数据不完整的情况，各种概率的转换达到估计该遗漏值。由于结合了先验知识，对于过度拟合的鲁棒性是很强的。 2.4 人工神经网络 神经网络，听着就有黑科技的感觉！其实他的原理也是很简单的，为什么叫做神经网络呢？其实它是模仿了神经元之间信息的传递。 说神经网络之前，先来说说最简单的一个模型：感知器（perceptron)。 感知器包含两个结点，一个是输入结点，一个是输出结点。每个输入结点通过一定的权值链接到输出结点。这个加权的链的作用是来模拟神经元间神经键链接的强度。训练一个感知器，就相当与不断的调整链接的权值，直到能拟合训练数据的输入输出为止。输出不单单是简单的加权，而是通过一个激活函数（activation function），选择不同的阈值分到不同的类。感知器常用的激活函数就是最简单的符号函数。感知器具体是如何学习的呢？通过一个学习率的加权乘以它的误差即可。注意对于感知器这个算法不一定会收敛(因为只有一层，类似只能用一个超平面来划分数据，像XOR分类的问题是不能通过一个超平面来划分的)。 知道了感知器那么就可以来看看ANN了。有什么区别呢？就是变得更复杂了，复杂在哪里呢？ 首先是输入层和输出层之前出现了很多隐藏层，根据隐藏层中神经元的连接方式，可以分为前馈神经网络（每一层的结点仅和下一层的结点相连）和递归神经网络（允许同一层结点相连，或者连接到前面的各层）。激活函数也变得更多样，例如Sigmoid函数，softmax等等。多层的神经网络是如何学习权值的呢？使用的Back-propagation的方法，反向传播。因为中间层没有确定的类标号，无法计算误差。 那么确定一个多层的神经网络，首先需要确定输入层的结点数，输出层的结点数，网络的拓扑结构（隐藏层结点数、前馈还是递归等等），初始权值和偏置（常常随机赋值，但是有可能陷入局部最小值）。 ANN可以处理冗余信息，但是对噪声非常敏感，而且其梯度下降的算法经常会收敛到局部最小值（在权值更新公式中加入一个动量项来解决），而且这个模型复杂，成本比较高。 2.5 支持向量机SVM 支持向量机是近年来工业中越来越常用的一种技术，相比较之前的模型我们首先来介绍一下最大边缘超平面的概念。 作为支持向量机的基础概念，这个概念非常好理解，超平面就是一种线性划分，想象一下两个数据集，如果它区分的足够清晰，我们可以用无数个超平面来将两个数据集分开，那么哪个超平面最好呢？就是最大超平面的概念。最大超平面具有最大的宽度，何谓宽度呢？就是平行于这个超平面向外扩张，最早不能将两类数据划分开的那两个平面之间的垂直距离。那么为什么宽度最大就最好呢？显而易见的是，如果这个宽度越大，当数据集的噪声越大时，越不容易受到干扰（注意这里面的逻辑）。如果他的决策边界的边缘比较小，就很敏感。 那么接下来就说说SVM了。最简单的SVM是一种线性分类器。线性SVM就是在寻找最大边缘超平面。如何寻找呢？我们假设这个最大超平面由一个线性函数决定。 w⋅x+b=0w\\cdot x+b=0w⋅x+b=0 www和bbb是模型的参数，xxx是训练集。 如果我们将某一类的取值定为1，另一类的取值定为-1，那么可以通过调整参数w,bw,bw,b是的这个最大超平面的边缘由如下两个方程决定： w⋅x+b=−1w\\cdot x+b=-1w⋅x+b=−1 w⋅x′+b=1w\\cdot x^{'} +b =1w⋅x​​′​​​​+b=1 两个超平面相减就得到边缘 d=(x−x′)=2∥w∥2d=(x-x^{'})=\\frac{2}{\\|w\\|^2}d=(x−x​​′​​​​)=​∥w∥​2​​​​2​​ 如何构建SVM模型呢？简单来说就是最大化变换d（因为我们在寻找最大边缘超平面），同时要满足两个条件： w⋅xi+b≥1w\\cdot x_i+b\\ge1w⋅x​i​​+b≥1 如果 yi=1y_i=1y​i​​=1 w⋅xi+b≤−1w\\cdot x_i+b\\le-1w⋅x​i​​+b≤−1 如果 yi=−1y_i=-1y​i​​=−1 那么这就变成了一个拉格朗日方程，但是他的求解并没有那么简单，需要转化成对偶问题，利用二次规划的方法求得拉格朗日乘子的值，然后在带回求得参数的值。 这个方法可以解决数据集可以用线性的超平面划分的情况下，如果不可分呢或者说如果只是牺牲了一点错误率而有更好的稳定性我们选择哪一个呢？ 我们定义软边缘的SVM其实是更常用的，软边缘就是在边缘方程的条件中加入了松弛变量： w⋅xi+b≥1−ξiw\\cdot x_i+b\\ge1-\\xi_iw⋅x​i​​+b≥1−ξ​i​​ 如果 yi=1y_i=1y​i​​=1 w⋅xi+b≤−1+ξiw\\cdot x_i+b\\le-1+\\xi_iw⋅x​i​​+b≤−1+ξ​i​​ 如果 yi=−1y_i=-1y​i​​=−1 引入松弛变量的作用就是允许原始的边缘中包含了一些错误的点，那么如果不对ξ\\xiξ的大小进行约束，那么这个错误率可能会变得很大，如何调整呢？简单，就对目标函数加入一个对ξ\\xiξ的惩罚项。 f(w)=∥w∥22+C(Σi=1Nξi)kf(w)=\\frac{\\|w\\|^2}{2}+C\\left(\\Sigma^N_{i=1}\\xi_i\\right)^kf(w)=​2​​∥w∥​2​​​​+C(Σ​i=1​N​​ξ​i​​)​k​​ 同样构建拉格朗日方程，转换成对偶问题，二次规划求解拉格朗日乘子，然后再解出参数即可。 以上介绍的都是线性的SVM技术，下面来说说非线性的SVM技术。 讲了这么多线性的情况，突然冒出来一个非线性，是不是很不爽，别担心，我们的思路是将非线性的数据映射到一个新的坐标空间，使得我们可以使用之前线性的方法来解决这个问题。那么这里的重点就转移到了如何做这个属性空间的转换呢？ 一个低维非线性的空间可以通过映射到一个高维的空间达到线性的要求，比如低维空间中的圆(x+1)2+(y+1)2=1(x+1)^2+(y+1)^2=1(x+1)​2​​+(y+1)​2​​=1映射到高维的线性空间(x12,x22,x1,x2,x1x2)(x_1^2,x_2^2,x_1,x_2,x_1x_2)(x​1​2​​,x​2​2​​,x​1​​,x​2​​,x​1​​x​2​​)。可以看出这种方法一个潜在的问题就是可能会产生维灾难！ 当当当当当！请叫我解决高维问题的核函数！核技术是一种使用原属性集计算变换后的空间中的相似度的方法。核函数（Kernel Function）是在原属性空间中计算变换后空间中的两个向量相似度（点积）的函数。核函数根据不同的形式可以分为多项式核函数、高斯核函数等。SVM核变换后的空间称作再生核希尔伯特空间（Reproducing Kernel Hilbert Space,RKHS）。使用核函数计算点积（即相似度）的开销更小，而且既然是在原空间内计算，维灾难问题自然就解决了。 这么好的技术为什么不广泛使用呢？当然会有一定的条件啦。这就是Mercer定理。符合Mercer定理就说明，存在一个相应的变换，使得计算一对向量的核函数等价于在变换后的空间中计算这对向量的点积。 Mercer定理 核函数K可以表示为： k(U,V)=Φ(u)⋅Φ(v)k(U,V)= \\Phi(u) \\cdot \\Phi(v) k(U,V)=Φ(u)⋅Φ(v) 当且仅当对于任意满足∫g(x)2dx\\int g(x)^2dx∫g(x)​2​​dx为有限值得函数g(x)g(x)g(x)，则 ∫K(x,y)g(x)g(y)dxdy≥0\\int K(x,y)g(x)g(y)dxdy \\ge0∫K(x,y)g(x)g(y)dxdy≥0 满足Mercer定理的核函数成为正定核函数，例如之前提到的多项式核函数、高斯核函数、Sigmoid核函数。 Ok，支持向量机也差不多圆满了。为什么SVM这么广泛的应用呢，因为它能找到全局最小点！而像神经网络、决策树等分类方法大多使用贪心算法，一般只能获得局部最优点。 Copyright © Niu 2015 ~ 2018 all right reserved，powered by GitbookUpdate Time: 2018-05-23 17:21:40 "},"shu-ju-wa-jue-dao-lun/fen-lei-2-0.html":{"url":"shu-ju-wa-jue-dao-lun/fen-lei-2-0.html","title":"分类2.0","keywords":"","body":"3.分类的组合方法 前文已经讲述了诸多分类的方法，决策树、神经网络、SVM、基于规则的分类器。这些都是一个单个的分类器，那么这节我们就来打破他们之间的界限，通过组合多个分类器来提高分类的准确性。这种技术称作组合，既然是个组合就得有个基分类器，然后通过对每个基分类器的预测进行投票来进行分类。 3.1 请问，为什么组合的更好？ 思考一个问题，如果法官认为你有罪的概率是0.35，那么当你重复25个独立的法官（每个认为你有罪的概率都是0.35）在一起采取投票(超过一半法官认为你有罪的时候你就定罪）的方式决定你有罪的概率是多少呢？ Σi=1325C25iϵi(1−ϵ)25−i=0.06\\Sigma_{i=13}^{25}C^i_{25}\\epsilon^i(1-\\epsilon)^{25-i}=0.06Σ​i=13​25​​C​25​i​​ϵ​i​​(1−ϵ)​25−i​​=0.06 如果你不能理解上面这个例子，想象一下，把每个法官看做一个分类器，把判罪的概率看做分类器的错误率，那么是不是容易懂了呢？ 可以看出组合分类器的性能高于基分类器。也不是时时刻刻都是这样的，这也需要条件：首先基分类器之前应该尽量相互独立，如果每个基分类器都完全一样，那么再怎么平均也都是徒劳。然后就是基分类器的误差率不能高于0.5，高于0.5就是说你不用分类器，瞎蒙一个的概率都比用这个分类器好... 3.2 这么好的方法如何构造呢？ 简单地说就是在数据集上构造多个分类器，当输入未知样本做预测时，让这么多的分类器投票选择。具体一点说有以下几种方法： 通过处理训练数据集。通过对原始数据抽样得到多个训练集，在每个训练集上建立一个分类器。常见的bagging和boosting。 通过处理输入特征。通过选择输入特征的子集来形成每个训练集。对于含有大量冗余特征的数据集非常好用，常见的如随机森林。 通过处理类标号。适用于类数足够多的情况。将类二分类，然后根据新的类建立分类器，然后重新二分类，再建立一个分类器，由此重复得到多个基分类器。常见的如：错误-纠正输出编码（error-correcting output coding）。 通过处理学习算法。很多学习算法都有很多变种，例如神经网络，不同层数的隐藏层，不同的激活函数，不用的权值。将不同方法的分类器得到组合就是这种方法的思想。 组合方法对于不稳定的分类器效果很好。不稳定的分类器对于训练集的微小变化很敏感。例如基于规则的分类器、决策树、神经网络等。 3.3 Bagging Bagging是一种根据均匀分布从数据集中重复抽样（有放回的）的技术。每个抽样的数据集都和原数据集一样大。和之前说过的Boosting类似，每个样本大约包含63.2%的原始数据。 在每个抽取的样本上执行一个基分类器，然后这时候来了一组陌生的数据，那么所有的基分类器就开个会，投票决定这个陌生的数据归属于哪一个类。Bagging为什么能提高分类的效果呢？因为它能够通过改善基分类器的方差来降低泛化误差。那么这也就说明，面对那些比较稳定的分类器，Bagging貌似不是一个好方法，因为稳定的分类器方差较小，一般偏倚较大。还有就是Bagging不太受过分拟合的影响。 3.4 Boosting 提升是一个迭代的过程，使用自适应的方法来改变训练样本的分布，使得基分类器聚集在那些很难分类的样本上。 简单的说Boosting的实现过程是：开始时，赋予每个样本同样的权值，使得他们等可能的被选作训练集。根据训练样本的抽样分布抽取一个新的样本集，然后在该训练集上训练一个分类器，并使用该分类器对原始数据中的所有样本进行分类。第二轮，将错误分类的样本提高权值，正确分类的样本减少权值，然后根据新的抽样分布抽取一个新的样本。重复以上过程即可。 Boosting有多种算法，主要区别在于权值更新的方法以及如何组合每个分类器。下面就介绍一下Adaboost。 Adaboost是常用的组合方法。以上的方法都是采用了多数投票选择的方法，那大家有没有想过，并不是每个分类器都是一样的好，就像玩狼人杀一样，村长这种重要角色是有两票的，而普通村民只有一票。所以Adaboost首先定义了每个分类器的重要性。重要性如何体现呢？误差率越小的基分类器，重要性就越高。可以看一下这些优美的公式。 误差率： εi=1N[Σj=1NwjI(Ci(xj)≠yi)]\\varepsilon_i=\\frac{1}{N}[\\Sigma^N_{j=1}w_jI(C_i(x_j)\\neq y_i)]ε​i​​=​N​​1​​[Σ​j=1​N​​w​j​​I(C​i​​(x​j​​)≠y​i​​)] 其中p为一个示性函数。 基分类器CiC_iC​i​​的重要性如下： αi=12ln(1−εiεi)\\alpha_i=\\frac{1}{2}ln(\\frac{1-\\varepsilon_i}{\\varepsilon_i})α​i​​=​2​​1​​ln(​ε​i​​​​1−ε​i​​​​) 为什么优美呢？大家可以把重要性的曲线图画出来。不仅如此，好不容易算出来的重要性，还可以用到权值的更新中！ wij+1=wi(j)Zj×{e−αj if Cj(xi)=yieαj if Cj(xi)≠yiw_i^{j+1}=\\frac{w_i^{(j)}}{Z_j}\\times \\begin{aligned} \\left\\{ \\begin{aligned} &e^{-\\alpha_j}\\ if\\ C_j(x_i)=y_i \\\\ &e^{\\alpha_j}\\ if\\ C_j(x_i)\\neq y_i \\\\ \\end{aligned} \\right. \\end{aligned}w​i​j+1​​=​Z​j​​​​w​i​(j)​​​​×​​⎩​⎨​⎧​​​​​​​​e​−α​j​​​​ if C​j​​(x​i​​)=y​i​​​e​α​j​​​​ if C​j​​(x​i​​)≠y​i​​​​​​ 其中ZjZ_jZ​j​​是一个正规因子，用来确保取值之和为1。 尽管Adaboosting有种种的优势，但是他总是趋于错误分类的样本，很容易导致过分拟合的影响。 3.5 随机森林 随机森林，有没有一种宠物小精灵进化的感觉？随机树到随机森林！其实随机森林就是一种专门为决策树分类器设计的组合方法。 随机森林与Bagging Adaboosting不同，他不抽选样本了，他抽选变量。从很多属性中随机选取一部分作为一个样本集，然后根据这些属性进行决策树分类，重复以上过程，得到多个基分类器。 选取属性集的方法大概有三种： 1.第一种是随机的选择F个特征，进行决策树分类。通常F=log2d+1F=log_2d+1F=log​2​​d+1，d是输入特征数。这种方法称为Forest-RI。 2.第二种方法面对特征数d比较小的情况。没有条件我们就创造条件，将不同属性的线性组合作为新的特征加入到抽样的过程中。这种方法称为Forest-RC。 3.第三种方法是，在决策树的每个结点，从F个最佳划分中随机选择一个。除非F非常大，否则这种方法会构建相关性更强的（相比于前两种）的树。 随机森林的性能取决于两点，一个是一组分类器的强度（即分类器的平均性能，能够正确预测给定样本的可能性），以及树之间的相关性。相关性越小，随机森林的性能越好。 随机森林的分类准确率可以与Adaboost相媲美，而且对噪声有更好的鲁棒性，并且计算速度更快。 4.不平衡类的问题 不平衡类的问题是十分常见的，比如信用卡欺诈、不合格产品检验等等。这些问题中，稀有类分类的准确率往往更有价值，然而由于是不平衡的问题，我们上面说过的分类器有可能就会出现问题。 4.1 不平衡类的度量 首先我们来来看看之前我们都是如何评价一个分类器的性能的，最常用的就是准确率了。那么对于不平衡类的问题，是否准确率能反映问题呢？想象这样一个场景，我们的银行系统预测信用卡合法交易的准确率是99%，但是只有1%的客户可能会有诈骗行为存在，那么这个模型可以说性能好吗？显然是不准确的。 那么我们就来看看如何度量不平衡类问题的分类器性能。 准确率度量将每个类看的同等重要，不适合分析不平衡类的问题。不平衡类问题中稀有类通常记为正类，而多数类记为负类。通常有如下混淆矩阵： 预测的类 预测的类 + - 实际的类 + f++(TP)f_{++}(TP)f​++​​(TP) f+−(FN)f_{+-}(FN)f​+−​​(FN) 实际的类 - f−+(FP)f_{-+}(FP)f​−+​​(FP) f−−(TN)f_{--}(TN)f​−−​​(TN) TP:True Positive 正确分类到正类中的正样本数 FP:False Positive 错误分类为正类的负样本数 TN:True Negative 正确分类到负类中的负样本数 FN:False Negative 错误分到负类中的正样本数 真正率（TPR,True positive rate）或者叫灵敏度（sensitivity）定义为被正确预测的正样本比例： TPR=TP(TP+FN)TPR=\\frac{TP}{(TP+FN)}TPR=​(TP+FN)​​TP​​ 真负率（TNR,True negative rate）或叫做特指度（specificity）定义为正确预测的负样本比例： TNR=TN(TN+FP)TNR=\\frac{TN}{(TN+FP)}TNR=​(TN+FP)​​TN​​ 同理假正率、假负率就预测为正类的负样本比例和预测为负样本的正样本比例。 重要的是一下概念： 召回率（recall）和精度（precision）。 精度p=TPTP+FPp=\\frac{TP}{TP+FP}p=​TP+FP​​TP​​ 召回率r=TPTP+FNr=\\frac{TP}{TP+FN}r=​TP+FN​​TP​​ 精度为分为正类中真正为正类的比例，而召回率是所有正类样本被真正预测出来的比例。 如何兼顾精确率和召回率呢，构建他们的调和平均数，即F1F_1F​1​​度量。 F1=21r+1pF_1=\\frac{2}{\\frac{1}{r}+\\frac{1}{p}}F​1​​=​​r​​1​​+​p​​1​​​​2​​ 更一般的有FβF_\\betaF​β​​度量 接下来就是分类器非常常用的一个评价标准，ROC（receiver operating characteristic）接受者操作特征曲线。它以真正率作为纵轴，假正率作为横轴，曲线中每个点对应一个基分类器。 对于ROC曲线，越靠近左上角的曲线越好（真正率高，假正率低）。一个随机猜测的分类器的ROC曲线是从左下角到右上角的一条直线，如果分类器的曲线低于这个曲线的话，那么这个分类器是十分差劲的（还不如瞎猜的概率高）。还有一种评判方式是通过AUC(area under roc)来评价，AUC越大说明分类器的性能越好。 4.2 代价敏感学习 既然已经知道用什么度量来评价不平衡类的问题了，那么具体怎么构建模型呢。就是这节的标题，代价敏感学习。首先说下代价矩阵，类似混淆矩阵，代价矩阵给每个分类情况一个编码，根据不同的情况，给予不同的权值，例如TP的情况我们可以给予-1的编码，FN的情况就比较严重了，给予它100的编码，而FP得情况，还可以接受，给予+1的编码，而TN无所谓了，我们就给与他0的编码，编码的大小代表错误的代价。用不同的编码乘以相应的TPR,TNR,FPR,FNR等等，可以得到整个模型总代价： Ct(M)=TP×C(+,+)+FP×C(−,+)+FN×C(+,−)+TN×C(−,−)C_t(M)=TP\\times C(+,+)+FP\\times C(-,+)+FN\\times C(+,-)+TN\\times C(-,-)C​t​​(M)=TP×C(+,+)+FP×C(−,+)+FN×C(+,−)+TN×C(−,−) 其中C(i,j)C(i,j)C(i,j)表示预测一个i类记录为j类的代价。 如何将这种代价信息加入到模型中间去呢？ 例如在决策树的过程中：可以用作选择分类数据的最好属性。决定子树是否需要剪枝。决定权值的收敛。修改每个叶节点上的决策规则。 4.3 基于抽样的方法 上面的方法是不是稍显麻烦呢？来看看从根源的角度上解决这个问题的方法（不一定会比代价敏感学习的方法好）。简单来说，不平衡类就是因为正样本太少，负样本太多。那么我们就这么做，正样本多抽一些，负样本少抽一些，使得二者达到一个平衡。需要用到两种技术： 1.不充分抽样（undersampling）。取负样本的一个随机抽样，与正样本混合产生新的数据集。这个方法一个明显的缺点就是可能会把有用的负样本记录删除掉。怎么解决呢？多次执行（类似组合方法的思想）或者使用聚焦的不充分抽样，即明确的排除一些负样本，如那些离决策边界很远的样本。 2.过分抽样。不断抽正样本，使得正样本数与负样本达到一个平衡。或者产生新的正样本，例如找到一个正样本的一个最邻近正样本点，然后在他们的连线中随机取一点作为新的正样本记录。当然，如果存在噪音数据的话，会导致模型的过分拟合。 既然都有缺点，不如折中一下，将两个方法混合，一起用。对正样本进行过分抽样，负样本进行不充分抽样就好了！ 5.多类问题 前面大多数的分类方法都侧重于二分类的问题，但是现实往往是残酷的，你得面对多分类的情况。怎么解决呢？ 1.将多分类问题分解，每一将其中一个类定义为正类，其他为负类，对所有类都这么做一遍，每一遍做一个分类器，然后就投票吧。或者将输出映射到一个概率空间，然后选出最大概率的类。（这样做是为了减少平局的可能） 2.与多分类问题中的每一对类都构建一个分类器。这个更复杂。 3.纠正输出编码（error-correcting output coding, ECOC）。前面两种方法对于错误都十分敏感，所以就出现了ECOC。简单来说就是给一个类一个长度相同的编码，有点类似二进制转化的思想。不过这个编码是通过信息论的理论确定的。然后对编码中每一个二进制做一个二分类器即可。 Copyright © Niu 2015 ~ 2018 all right reserved，powered by GitbookUpdate Time: 2018-05-23 17:40:40 "},"shu-ju-wa-jue-dao-lun/emsuan-fa.html":{"url":"shu-ju-wa-jue-dao-lun/emsuan-fa.html","title":"EM算法","keywords":"","body":"数据挖掘十大算法之一的EM（exception maximization）算法的大名可是如雷贯耳，EM算法简单来说，就是由两部分组成：E和M。E是exception，即求期望；M是maximization，求极大值。 为什么要使用EM算法呢？ 我们常见的概率模型一般都只有观测变量，即都是给定的数据，可以直接用极大似然估计、贝叶斯估计等方法。但是有些时候，概率模型是含有隐变量或潜在变量的，这时候就是EM算法发挥的时刻了。 先来说说隐变量，隐变量就是无法直接从给定的数据中观察到的变量，比如「统计学习方法」一书中所说的例子：三个筛子A,B,C进行如下实验，先投掷A硬币，根据其结果选出B或C硬币，比如正面选B，反面选C，然后掷出选择到的硬币，如果出出现正面记作1，出现反面记作0，独立重复n次，得到观测结果如下： 1,1,0,1,0,0,1,0,1,1 那么如何估计三个硬币正面出现的概率呢？ 从结果来看，只能看到B和C的结果，而A得概率是无法直接得到的，此时A正面的概率就是一个这个模型中一个隐变量。 一般的，用Y=(Y1,Y2,...,Yn)TY=(Y_1,Y_2,...,Y_n)^TY=(Y​1​​,Y​2​​,...,Y​n​​)​T​​表示观测到的随机变量的数据，Z=(Z1,Z2,...,Zn)TZ=(Z_1,Z_2,...,Z_n)^TZ=(Z​1​​,Z​2​​,...,Z​n​​)​T​​表示隐随机变量的数据。YYY和ZZZ在一起被称为完全数据，观测数据YYY又称为不完全数据。参数空间为θ\\thetaθ，假设观测数据Y的概率分布是P(Y∥θ)P(Y\\|\\theta)P(Y∥θ),那么其概率分布是P(Y∥θ)P(Y\\|\\theta)P(Y∥θ)，假设YYY和ZZZ的联合概率分布是P(Y,Z∥θ)P(Y,Z\\|\\theta)P(Y,Z∥θ)，那么完全数据的对数似然函数就是logP(Y,Z∥θ)logP(Y,Z\\|\\theta)logP(Y,Z∥θ)。 EM算法的步骤就是： 输入：观测数据YYY，隐变量ZZZ，联合分布P(X,Y∥Z)P(X,Y\\|Z)P(X,Y∥Z)，条件分布P(Z∥Y,θ)P(Z\\|Y,\\theta)P(Z∥Y,θ) 输出：模型参数θ\\thetaθ 选择参数的初值θ(0)\\theta^{(0)}θ​(0)​​,开始迭代 E步：记θ(i)\\theta^{(i)}θ​(i)​​为第iii次迭代参数θ\\thetaθ的估计值，在第i+1i+1i+1次迭代的E步，计算 Q(θ,θ(i))=Ez[logP(Y,Z∥θ)∥Y,θ(i)]=ΣzlogP(Y,Z∥θ)P(Z∥Y,θ(i))\\begin{aligned} \\begin{aligned} Q(\\theta,\\theta^{(i)}) &=E_z[logP(Y,Z\\|\\theta)\\|Y,\\theta^{(i)}]\\\\ &=\\Sigma_zlogP(Y,Z\\|\\theta)P(Z\\|Y,\\theta^{(i)}) \\end{aligned} \\end{aligned}​​Q(θ,θ​(i)​​)​​​​=E​z​​[logP(Y,Z∥θ)∥Y,θ​(i)​​]​=Σ​z​​logP(Y,Z∥θ)P(Z∥Y,θ​(i)​​)​​​​ M步：求使得Q(θ,θ(i))Q(\\theta,\\theta^{(i)})Q(θ,θ​(i)​​)极大化的θ\\thetaθ，确定第i+1i+1i+1次迭代的参数值θ(i+1)\\theta^{(i+1)}θ​(i+1)​​ θ(i+1)=argmaxθQ(θ,θ(i))\\theta^{(i+1)}=argmax_{\\theta}Q(\\theta,\\theta^{(i)})θ​(i+1)​​=argmax​θ​​Q(θ,θ​(i)​​) 重复第2步和第3步，直到收敛。 其中第二步的Q函数（Q function）是EM算法的核心。 需要注意的是，大多数情况下EM算法是收敛的，但是不能保证收敛到全局最优，可以多选几个初值，通过比较最后得到的参数，选定一个最符合实际的结果。高斯混合模型以及隐马尔科夫模型都是EM算法的重要应用，主要应用于含有隐变量的概率模型的学习。 EM算法其实还可以解释为F函数的极大-极大算法。EM算法的一次迭代可以由F函数的极大-极大算法实现。而EM算法也有很多自己的推广，例如GEM（generalized exception maximization）。GEM也包含多种算法，例如使用F函数的算法，有的时候Q函数的极大化是不那么容易求出来的，所以我们放松EM算法的第3步，将极大化放松为，只要比原来的似然函数大即可，或者换一种思路，我们不直接求θ\\thetaθ使得Q函数最大，而是把θ\\thetaθ中每一个分量，依次求使得Q函数最大的结果，然后组合起来（具体来说就是，首先除了第一个的分量，保持其他值不变，求解使得Q函数极大的第一个分量，然后替换到原始的第一个分量，然后除了第二个分量，其余分量保持不变，注意此时第一个分量已经改变了，然后求解使得Q函数极大的第二个分量，替换原始的第二个分量，依次类推，知道所有分量都已经被替换了一遍）。 Copyright © Niu 2015 ~ 2018 all right reserved，powered by GitbookUpdate Time: 2018-05-23 17:27:29 "},"macpei-zhi-si-ren-zhi-nan.html":{"url":"macpei-zhi-si-ren-zhi-nan.html","title":"MAC配置私人指南","keywords":"","body":"MAC配置指南 APP LIST gitbook 个人资料积累及wiki系统 dash 代码查询工具 iterm 终端查询工具 xmind zen 思维导图工具，结构化资料 sublime 文本编辑器 alfred 更好用的spotligh focus timer 番茄工具发工具 mpv 视频工具 evernote 资料积累工具 eudic 字典 anyconnect vpn链接 karabiner 外接键盘神器 noizio 白噪声工具 calibre 文本格式转换工具 SUBLIME PACKAGE Alignment Color Scheme AutoFileName BracketHighlighter Codecs33 ConvertToUTF8 Emmet File History FileDiffs FileHeader Markdown Extended MarkdownEditing MarkdownTOC OmniMarkupPreviewer SideBarEnhancements SublimeCodeIntel SublimeLinter Copyright © Niu 2015 ~ 2018 all right reserved，powered by GitbookUpdate Time: 2018-05-22 20:50:34 "}}