{"./":{"url":"./","title":"Introduction","keywords":"","body":"Notes Notes for my readings and thinkings. Copyright © Niu 2018 all right reserved，powered by GitbookUpdate Time 2018-05-03 14:35:23 "},"esl-reading-notes.html":{"url":"esl-reading-notes.html","title":"ESL Reading Notes","keywords":"","body":"Elements of Statistical Learning elements of statistical learning 是上学的时候就开始看的一本书，刚开始读的非常吃力，很多概念或者公式不是非常理解。现在已经工作一年了，断断续续还在看这本书，明显能够感觉到对不熟悉的算法的理解深刻了许多，但是这本书给我带来的知识并没有在实际应用中有较多的体现，继续前行吧。 Copyright © Niu 2018 all right reserved，powered by GitbookUpdate Time 2018-05-03 14:35:23 "},"esl-reading-notes/chapter-2.html":{"url":"esl-reading-notes/chapter-2.html","title":"Chapter 2","keywords":"","body":"Overview of Supervised Learning 本章主要是概论章节，讲述了统计学习的基础概念与问题。 2.2 变量类型 变量的类型：定序 定类 定距 定比 2.3 最小二乘与KKK邻近 最小二乘法的假设更加严格，而结果更加稳定，但是偏差可能比较大。 K邻近的假设非常少，结果比较准确（随着k的变小，结果越来越精确），但是其稳定性较差（随着k的升高，越来越稳定） 2.3.1 最小二乘法 最小二乘法做分类时更适合数据来源于Bivariate Gaussian分布，并且相互不相关，拥有不同的均值（此情形之后称为情景一） 2.3.2 K邻近 K邻近算法用目标x周围最近的k个x的函数值的平均来表示目标x的值。 Y^(x)=1k∑xi∈Nk(x)yi\\hat{Y}(x)=\\frac{1}{k}\\sum_{x_i \\in N_k(x)}y_i​Y​^​​(x)=​k​​1​​∑​x​i​​∈N​k​​(x)​​y​i​​ 通过邻近的这个概念是一种距离的测度，常用的距离为欧几里得距离。 K邻近的方法更适用于数据来源比较复杂的情况，例如来源于10个混合的低方差高斯分布，其中每个高斯分布的均值来源于同一个高斯分布(此情形之后称为情景二)。 K邻近方法需要计算的有效参数值个数大多数情况下比最小二乘法多，而且不能使用损失函数来惩罚。 k的大小有一个trade-off在里面，虽然k越小越精确，但是稳定性会越来越差，所以要选择合适的k。 2.3.3 最小二乘与K邻近的比较 一个小的总结： 最小二乘的方法得到的decision boundary是非常平滑的，它依赖于比较强的假设条件，得到的结果一般来讲variance较小，而bias比较高。更适用于情景一。 K邻近方法不需要数据的强假设，可是适用于任何情形，虽然其得到的结果非常精确，既Bias较低，但是其variance一般是比较大的。更适用于情景二。 2.4 统计决策理论 什么是最优的预测值？ 2.4.1 输出是连续变量的情况 如果使用MSE作为Loss Function的话，那么最好的预测就是基于观测值的条件期望。 Nearest-neighbor的方法更直接，它有两个近似： 1.用平均值替代均值 2.用目标x周围的邻近点来替代条件期望。 随着样本量N的增加，目标x的邻近点离x越来越近。随着k的增加，均值也越来越稳定。也就是说随着N和k的增加，K邻近的预测值将趋近于利用MSE作为损失函数的条件期望。 虽然如此，但是随着dimension p的增加，收敛依旧成立，其收敛速度会随着p的增加而减慢。 线性回归的方法直接将regression function近似为线性的。 虽然k邻近和最小二乘方法都是利用平均化的思想来估计条件期望，但是他们的假设却别非常大： 最小二乘法假设f(x)总体是一个线性的方程 k邻近假设f(x)又一个局部的常数方程决定 2.4.2 输出为分类变量的情况 在输出变为离散的情况下，我们定义一个Loss Function来惩罚分类出现的错误。再对所有类的Loss Function的求期望，使得这个期望最小的分类就是所估计的分类结果。 通过求解发现，结果很容易理解为，在给定了输入的情况下，使得出现类的概率最大的一类即所估计的类。这个分类方法就是Bayes classifier，而bayes classifier分类的错误率就成为bayes rate。 对于k邻近的方法，与连续的情况类似，利用平均值来近似期望， 利用x周围k个邻近点来近似条件希望。 2.5 高维下的局部方法 虽然KNN方法在样本量增大的情况下有着较好的逼近性质，但是在高维情况下也会有许多问题出现。 高维情况下，局部的方法将不怎么局部。 r1/pr^{1/p}r​1/p​​ 高维情况下，样本点更趋向于集中于样本的边界。 高维情况下，实际的样本将变得非常稀疏。 2.6 统计模型、监督学习、函数估计 2.4节我们已经知道了，通过最小化Loss Function的期望来估计模型函数f(x)，常见的有最小二乘法和KNN，但是这种方法也有一定的缺陷： 如果是在高维情况下，KNN方法与估计点的距离不如预想中的近（2.5节 第二种情形） 如果输入的数据有特殊的结构存在，那么就可以同时降低偏差和方差。 2.6.1 统计模型 Additive error model Y=f(X)+ϵY=f(X)+\\epsilonY=f(X)+ϵ 随机误差项ϵ\\epsilonϵ是零均值的，而且与XXX独立。 这个模型适用于输出为连续型变量，而对于离散型（定性变量）更加直接，f(X)f(X)f(X)直接用Pr(G∥X)Pr(G\\|X)Pr(G∥X)代替即可。 2.6.2 监督学习 Supervised learning attempts to learn fff by example through a teacher.(ESL page 29) 2.6.3 函数估计 最大似然估计 2.7 结构化回归 可以看出局部方法有诸多缺陷，为了解决这些问题，出现了结构化回归方法。 2.7.1 方法的难点 考虑到RSS条件： RSS(f)=∑i=1N(yi−f(xi))2 RSS(f)=\\sum_{i=1}^{N}(y_i-f(x_i))^2RSS(f)=∑​i=1​N​​(y​i​​−f(x​i​​))​2​​ 只要经过点(xi,yi)(x_i,y_i)(x​i​​,y​i​​)的f(x)f(x)f(x)都是RSS的一个解，可见这种解有无数多个，为了确定一个更小范围的解，就需要多加一些限制条件。 没有哪一种解法是最优的解法，要根据不同的情况来自己决定。 通常来说，所有解法都是基于同一个思想，对于距离x足够小的范围内的点实行同一种规则，例如近似常数、线性或者低阶多项式等等。 限制的力度取决于neighborhood size，规模越大，约束的能力越强，同时也表明该方法对不同的约束会更敏感。 [?] 限制的性质一方面也取决于所使用的metric。可以简单分为explicitly和implicitly两种。 有一点必须知道的是，如果在任何各项同性的领域内使用可变的方程，那么同样会产生高维的问题。并且，所有克服了高维问题的方法基本上都不允许领域在各个维度上同时变小。 2.8 约束的方法分类 2.7节所说的限制的种类大致可以分为几个类，而大多数方法都可以归结在这几类或这几类的组合中。 2.8.1 添加惩罚项的方法 这种方法有时候也称为正则化方法。 2.8.2 核函数方法 核方程对于目标x周围的x给予不同的权重。 2.8.3 基函数方法 这个是对线性模型的推广，假设模型仍然是可加的，对系数而言仍然是线性的，但是所加的这些项则变成了函数（基函数），对这种方法而言，比较常用的一个是样条，另外一个是多项式回归。 2.9 模型选择 模型选择要基于test sample，主要是在bias与variance的tradeoff中选择一个度，使得expected prediction error达到最小。 Copyright © Niu 2018 all right reserved，powered by GitbookUpdate Time 2018-05-16 15:37:59 "},"esl-reading-notes/chapter-3.html":{"url":"esl-reading-notes/chapter-3.html","title":"Chapter 3","keywords":"","body":"Linear Regression Models and Least SquaresIntroduction Copyright © Niu 2018 all right reserved，powered by GitbookUpdate Time 2018-05-03 14:35:23 "},"shu-ju-wa-jue-dao-lun.html":{"url":"shu-ju-wa-jue-dao-lun.html","title":"数据挖掘导论","keywords":"","body":"数据挖掘导论是一本介绍型的入门书籍，比较好的解释了基础的机器学习算法，是一本纯理论的书籍，相比较推荐较多的「机器学习实战」、「集体智慧编程」等等来说缺少coding的训练。 本书主要包含5个大方面的内容： 数据 分类方法 关联分析 聚类方法 异常值检测 数据是基础章节，主要介绍了数据的类型、基础变换方法及描述方法等。 剩下的章节无前后顺序，可以按照自己的重点学习，如果只是入门建议按照分类 -> 聚类 -> 关联分析 -> 异常值检测的顺序阅读。 Copyright © Niu 2018 all right reserved，powered by GitbookUpdate Time 2018-05-10 16:52:57 "},"shu-ju-wa-jue-dao-lun/shu-ju.html":{"url":"shu-ju-wa-jue-dao-lun/shu-ju.html","title":"数据","keywords":"","body":"1.概述 数据挖掘已然是当下做数据的人不能不知道的知识了，那么数据挖掘到底是什么呢。个人理解就是在数据中发现有价值的信息。数据挖掘的任务大概可以分为预测和描述两大类。预测包括我们常说的回归与分类，前者针对于目标变量是连续的情况，而后者针对目标变量是离散的情况。描述是做什么呢，就是概括数据中潜在的联系模式，比如相关性、趋势、聚类、异常等等。常用的技术包括分类、聚类、关联分析、异常检测等。 2.数据 说数据挖掘的技术之前，先得说说数据。巧妇难为无米之炊，数据对于数据分析人员的重要性不言而喻。罗老师讲过，数据分析就像做菜一样，首先得有材料（数据），然后还得洗菜（数据清洗、探索），然后得有菜谱（统计、机器学习模型），然后选择对的菜谱做菜（构建模型），最后摆摆盘（数据可视化，生成分析报告），再就是洗盘子了（规整资料）。作为基础，对于数据质量、类型的了解，对其处理的方式以及可用的模型是一个数据分析员具备的基本知识。 首先就是我们统计一开始就会学到的数据的类型：定类、定序、定距、定比。不多说了。而对于每种类型数据常用的方法可以归纳为： 类型 方法 变换方法 定类 众数、熵，列联相关 一对一变换 定序 中位数、百分位数、秩相关、游程检验、符号检验 单调变换 定距 均值、方差、皮尔逊相关系数、t和F检验 正彷射变换 定比 几何平均数（环比）、调和平均数（相对变化率）、百分比变差 scalar 同时数据按照不同的分类标准也可以分为：连续数据、离散数据。对称数据、非对称数据（文档词频矩阵）。 一般在观察一个数据集的时候考虑三个方面的特性：维度（变量个数），稀疏性，分辨率。通过这三个方面来了解这个数据集的基本特性。 下面说说数据质量的问题，数据的质量关乎整个数据分析结构的优劣，数据的质量主要由这几个方面构成： 测量误差和数据收集错误 测量误差在一定程度上是无法避免的，有可能是系统的也有可能是随机的。收集错误，一般都可以很好的纠正。 噪声和伪像 噪声是测量误差的随机部分，无法避免。一个算法的鲁棒性就是来衡量这个算法是否能应对噪声的干扰。数据错误或者确定性的失真就是伪像。 精度、偏倚、准确率 离群点 也称为异常值，离群点与噪声不同，是人们感兴趣的对象，如：信用卡欺诈。 缺失值 缺失值常用的处理方法： 删除数据或者属性；估计缺失值（众数、中位数、有分布的随机抽样）；忽略缺失值。 为了让数据更适合模型，为了改善模型、减少时间、降低成本、提高质量，常常需要对数据进行预处理。 根据需要处理的数据规模，对碎片化的数据进行规整。例如我只需要年销售额的数据，而只有销售额的数据，就需要事先进行聚集。 抽样。 选取具有代表性的样本。 常用的抽样方法有：简单随机抽样、分层抽样、整群抽样、有序抽样等等。 确定正确的样本容量是一个麻烦事。可以采用渐进抽样的方法（可以理解为逐步试验），逐步增加样本容量，观察结果的准确性，当当本容量的增加使得结果准确定的增幅小于一个可以接受的阈值时，即可以停止，确定样本容量了。 降维。Curse of Dimensionality听着名字就觉得太可怕了。降维也是近代统计研究的一个重要方向。在数据预处理的阶段我们就可以开始着手处理这个问题了。降低了维度首先会提高算法的性能，再者更容易理解（人对抽象空间的理解能力有限），而且可以方便与可视化。最最常用的线性技术就是大名鼎鼎的PCA了，还有他的好兄弟SVD。针对时间序列的数据，莫过于傅里叶变换了，以及小波变换。 还有一种方法就要对数据的含义有很好的理解了，这就需要这个领域的专家才可以。比如人为的特征提取，处理一个问题，专家觉得这个问题的主要影响方面有哪几个就把哪几个变量作为输入。还有新特征的构造，也需要专家的指点，比如区分一堆金属，我们有他们的质量与体积，这两个变量不好利用，我们就创建一个新的变量：密度，这就是特征的构造。 离散化。对于很多分类的模型，他们的目标变量往往是连续的，如何正确的将连续变量映射到离散空间也是有学问的。如果是非监督的方法，我们常用的就是分个区间，把对应的数标记为这个区间的类型，区间的划分主要有：等宽方法，等频率方法，以及K-means等聚类方法。监督的方法就可以使用熵来定义，先分成两部分，是的熵最小，取较大熵区间再分，循环往复。 不仅仅连续变量要离散化，离散的变量有时候还需要二元化。（为什么是二元化呢？因为常见的模型都是这样简化的。比如决策树）这时候就能体现出进制的伟大了！（6个人60桶酒问题）不过这种情况下常常会出现共线性的问题（类似dummy variable的情况）。或者就直接使用dummy variable。 根据自己经验的总结，感觉如果不是非常在乎度量的大小的话，尽量先标准化数据。（此条结果有待验证，有错误希望大家指出） 接下来再说说相似性、相异性的度量，为什么把这个也放在预处理这个阶段呢？因为许多算法例如聚类，KNN，异常检测等都需要相似相异的概念来衡量数据之间的亲疏远近，例如做系统聚类时，当计算出距离矩阵时就不需要原始数据了。如果不需要复杂的度量，可以参考下表： 定类数据： 相异度：dummy variable(相同取1，不同取0) 相似度：类似示性函数 定序数据： 相异度：d=∣x−y∣n−1d = \\frac{| x - y |}{ n - 1 }d=​n−1​​∣x−y∣​​ 相似度：s=1−ds = 1 - ds=1−d 定距或定比数据： 相异度：d=∥x−y∥d=\\|x - y\\|d=∥x−y∥ 相似度：s=−d,s=11+d,s=e−d,s=1−d−mindmaxd−minds = -d,s = \\frac{1}{1 + d},s = e^{-d},s = 1 - \\frac{d - min_d}{max_d - min_d}s=−d,s=​1+d​​1​​,s=e​−d​​,s=1−​max​d​​−min​d​​​​d−min​d​​​​ 面对复杂的相异度我们经常使用距离这个概念，常用的距离有欧几里得距离，其实是明可夫斯基距离的一种特殊形式，也是L2范数。还有常用的就是L1范数，上确界距离。还有包括马氏距离也比较常用。 复杂的相似度度量。如果数据是离散的可以使用混淆矩阵。基于混淆矩阵的概念有简单匹配系数（SMC），Jaccard系数（面对非对称的数据），以及余弦相似度（对象规范化，减少计算时间，非对称数据常用，量值重要时不要用）。面对连续的数据，可以使用常用的皮尔逊相关系数，其中可以选中中位数、绝对差、trim均值作为改进方法。 距离的选择还可以使用加权的方法，有点类似核函数的意思。 3.探索数据 数据清洗好了，先看看数据大概是个什么样子吧。这个阶段常用的技术就是汇总（计算均值方差），可视化（画个矩阵散点图看看相关性）还有不怎么明白的OLAP。 数据汇总学过统计概率论的人都十分明白啦，就说受可视化吧。 低位数据的可视化就是统计中常用的一些方法，非常好用。包括，茎叶图、直方图、箱线图、饼图、散点图等。 高维数据的可视化就不能仅仅使用坐标这个概念了，可以加入颜色、大小、形状等等来展示不同的维度。还有非常嘻哈的Chernoff脸！ OLAP不怎么懂，不多说了，大家可以自行查找文献。 Copyright © Niu 2018 all right reserved，powered by GitbookUpdate Time 2018-05-22 20:04:39 "},"shu-ju-wa-jue-dao-lun/guan-lian-fen-xi.html":{"url":"shu-ju-wa-jue-dao-lun/guan-lian-fen-xi.html","title":"关联分析","keywords":"","body":"关联分析 1.概述 关联分析用于发现隐藏在大型数据集中的有意义的联系，所发现的联系可以用关联规则或频繁项集的形式表示。 进行关联分析的时候，需要处理两个关键问题： 从大型事务数据集中发现模式可能在计算上要付出很高的代价。 所发现的某些模式可能是虚假的，因为他们可能是偶然发生的。 接下来的讨论从上面两个问题展开，首先来说说第一个问题，解释关联分析相关的概念以及有效的挖掘这种模式的算法。 2.问题的定义 前面简单说明了关联分析的定义，下面展开说明一下什么是关联规则。 2.1 二元表示 关联分析面对的是什么样的数据呢？ 一个最最常见的例子就是啤酒尿布的故事，这也是关联规则的一个应用，面对这样的数据我们在「数据」一文中讲到是购物篮数据。购物篮数据有两个维度，一个是事务，一个是项集。购物篮数据类似列联表，分横纵两个维度，每一行代表一个事务，每一列代表一个项。那么购物篮里面的数据是什么样子呢？就是常见的二元变量（0，1变量），即该项是否出现在了该事务中，通常这样野蛮的表示一个事件是有问题的，就是非对称的问题，因为我们通常认为项在事务中出现比不出现更重要。同时，这样简单粗暴的用0、1表示，常常忽略数据某些重要方面，比如数量、价格等等。 2.2 支持度（support）&置信度（confidence） 先谈一个概念叫做支持度计数，即包含特定项集的事务个数。若以T=t1,t2,...,tNT={t_1,t_2,...,t_N}T=t​1​​,t​2​​,...,t​N​​表示所有事务的集合，以σ\\sigmaσ表示支持度计数，那么有以下关系： σ(X)=∥{ti∥X⊂ti,ti∈T}∥\\sigma(X) = \\|\\{t_i \\| X \\subset t_i, t_i \\in T \\}\\|σ(X)=∥{t​i​​∥X⊂t​i​​,t​i​​∈T}∥ 其中 ∥⋅∥\\|\\cdot\\|∥⋅∥ 表示集合中元素的个数。 那么什么是支持度和置信度呢？ 简单来说就是概率or频率（支持度）和条件概率（置信度）的概念。 支持度和置信度是用来描述关联规则的度量，关联规则是形如X→YX \\rightarrow YX→Y的表达式，X、Y都是某个项集，而且是不相交的项集。 支持度用来确定给定数据集的频繁程度，支持度确定Y在包含X的事务中出现的频繁程度。 s(X→Y)=σ(X∪Y)Ns(X \\rightarrow Y)=\\frac{\\sigma(X \\cup Y)}{N}s(X→Y)=​N​​σ(X∪Y)​​ c(X→Y)=σ(X∪Y)σ(X)c(X \\rightarrow Y)=\\frac{\\sigma(X \\cup Y)}{\\sigma(X)}c(X→Y)=​σ(X)​​σ(X∪Y)​​ 支持度用来保证规则的出现并不是偶然的，通常用来删除无意义的规则。置信度保证规则的可靠性。 Tips : 关联规则的推论并不必然蕴含因果关系，指标是规则前件与后件的项明显地同时出现。因果关系需要利用常识或其他知识补充来判断。 2.3 定义关联规则 关联规则怎么做呢？就一句话，关联规则就是找出支持度大于等于 minsup 并且置信度大于等于 minconf 的所有规则。其中 minsup 和 minconf 是对应支持度和置信度阈值。 是不是很简单，但是具体怎么做呢？要找到每一条规则然后分别计算置信度和支持度吗？ 这确实是最简单粗暴的方法，但是其计算代价巨大。对于一个包含ddd个项的数据集中提取的可能规则的总数为： R=3d−2d+1+1R = 3^d - 2^{d+1} +1R=3​d​​−2​d+1​​+1 为了避免不必要的计算，可以事先对规则进行剪枝。 首先我们将关联规则的算法分成两步，首先计算其支持度然后再计算其置信度。认真点来说就是以下两步： 频繁项集产生。目标是发现所有满足最小支持度的项集，这些项集成为频繁项集。 规则的产生。 目标是从上一步发现的频繁项集中提取满足最小置信度的所有规则，这些规则称为强规则。 通常来说，频繁项集产生的计算开销远大于生产规则所需的计算开销。接下来将详细描述这两个关联分析算法中的关键步骤。 3.频繁项集的产生 假如我们执意使用枚举的方法产生频繁项集，那让我们来算一算。 对于包含k个项的数据集可能产2k−12^k - 12​k​​−1个频繁项集，不包含空集在内（C_k1+C_k2+C_k3+...+C_kkC\\_{k}^{1}+C\\_k^2+C\\_k^3+...+C\\_k^kC_k​1​​+C_k​2​​+C_k​3​​+...+C_k​k​​)。而随着k的增大，这个计算量可以说是指数级的增长，需要搜索的空间也随之指数级的增长。那么我们来看看计算量具体是多少， 假如对于上述的k个项的数据集而言一共有N个事务与之对应，最大事务宽度是w，那么通过比较每个事务与每个可能项集的时间复杂度是O(NMw)，其中M=2k−1M=2^k-1M=2​k​​−1是候选集数。 上面的时间复杂度中w是无法改变的事实，那么我们通过优化N、M来降低频繁项集产生的计算复杂度。 对于M，即减少候选项集的数目而言，我们采用先验原理（apriori）。对于减少比较次数而言，我们采用更高级的数据结构或者存储候选项集或者压缩数据集，接下来我们将详细说明这两个方面。 3.1 先验原理 先验原理: 如果一个项集是频繁的，则它的所有子集也一定是频繁的。 这个原理非常好理解，如果一个项集是频繁的，那么它的子集的支持度一定是大于它自己的，因为子集被包含的事务个数一定是更多的，所以如果一个项集是频繁的，那么它的所有子集也一定是频繁的。这个性质也称为支持度度量的反单调性。同样的它的逆否定理同样成立。 先验原理的逆否定理：如果一个项集是非频繁的，那么它的所有超集也一定是非频繁的。 这种基于支持度度量修剪指数搜索空间的策略成为基于支持度的剪枝。 3.2 Apriori算法的频繁项集产生 Apriori算法使用了基于支持度的剪枝方法来控制候选集的指数增长。具体步骤如下： 扫描所有数据，得出所有单项集的支持度。 将上一步中所有满足最小支持度的频繁项集进行组合得到2-项集。 循环第二步直到没有新的频繁项集产生。 该算法有两个特点：第一，它是一个逐层算法，从频繁1-项集到最长的频繁项集，需要遍历所有长度项集的每一层；第二，它使用产生——测试的策略来发现频繁项集，每次迭代都通过前一次迭代产生的频繁项集产生，然后计算这次迭代所有项集的支持度并与minsup比较。 下面就算法中的具体细节展开说明。 3.2.1 候选的产生与剪枝 上面一小节说到Apriori算法中第二步是产生后算计并且对不满足最小支持度的项集进行剪枝，在这个过程中需要注意的是产生候选集的要求： 避免产生太多不必要的候选集。候选集必须保证其所有真子集（这里我感觉书中的意思不是全部的真子集，而是全部的k-1项集，k是当前项集的宽度，因为关联规则的方法是逐层算法，只需要保证k-1阶即可保证全局最优解？）都是非频繁的，这样可以保证该候选集是频繁项集。 保证候选集的集合是完全的，即必须包含所有应该出现的频繁项集。 避免产生重复的候选集。例如一个4-项集{a,b,c,d}的产生可能有多种：{a}和{b,c,d}；{b,c}和{a,d}等等。 为了满足上述要求我们简述集中产生候选集的方法： 蛮力方法。 蛮力方法将所有的k-项集都看作候选集，然后再使用剪枝的方法去除不必要的项集。乍一看确实非常简单，但是计算开销巨大，下面来看看其时间复杂度。 假设一共有d个项，那么使用蛮力方法可以产生C_dkC\\_d^kC_d​k​​个k-项集集，每一个候选集可能产生k个k-1项集，然后每个k-1项集需要d个项比较来确定是否其真子集都是频繁项集。总结来说，这种方法的时间复杂度是O(Σ_k=1dkC_dk)=O(d⋅2d−1)O(\\Sigma\\_{k=1}^dkC\\_d^k)=O(d\\cdot2^{d-1})O(Σ_k=1​d​​kC_d​k​​)=O(d⋅2​d−1​​)。 可以看出这是一种指数型复杂度，非常\"蛮力\"！ F_k−1×F_1F\\_{k-1} \\times F\\_1F_k−1×F_1 方法。 这种产生候选集的方法是利用频繁1-项集来扩展每个(k-1)-项集。这种方法将产生O(\\|F_{k-1}\\|\\|F_1\\|)个候选k-项集。这种方法的复杂是O(Σk∥Fk−1∥∥F1∥)O(\\Sigma_k\\|F_{k-1}\\|\\|F_1\\|)O(Σ​k​​∥F​k−1​​∥∥F​1​​∥)。 相较于蛮力方法复杂度降低了不少，但是同样的非常容易产生重复候选项集。解决重复的方法可以利用字典编号的方法，将所有项进行编号，在进行k-项候选集的产生时，只选取比k-1项集中所有项编号都大的项进行组合，这样就可以避免重复问题。 该方法另外一个缺点是，仍然会产生不必要的项集，例如将频繁2-项集{a,b}和c结合得到3-项集{a,b,c}，但是其真子集{a,c}却不是频繁2-项集，这样就会导致该3-项集也是非频繁项集。这样的情况下，我们的解决方法是：对于每一个幸免于剪枝的候选k-项集来说，它的每一个项必须至少在k-1个(k-1)-项集中出现，否则，该候选集就是非频繁的。解释一下就是，k-项集的所有(k-1)-项集子集必须出现在k-1频繁项集中，k项集中的每个元素起码出现两次才能保证其k-1项集有可能是频繁的，假如某个项只出现了一次，那么其必定有一个k-1项集的组合是非频繁的。这个方法可以减少非频繁项集的产生，但是不能保证所有通过此方法得到的候选项集一定是频繁的。 Fk−1×Fk−1F_{k-1}\\times F_{k-1}F​k−1​​×F​k−1​​方法。 这种候k项候选集产生的方法是合并一对频繁(k-1)-项集，仅当它们的前 k-2 个项都相同的时候。 使用这种方法的一个先决条件是，候选项集产生的过程中必须使用了字典序方法进行组合。 为什么一定要前 k-2 项都相同呢，和 Fk−1×F1F_{k-1} \\times F_1F​k−1​​×F​1​​ 方法中剪枝的思想类似，我们必须要求产生的k项集的所有k-1项集都是频繁的，那么当前k-2项都相同的时候，必然能保证其所有k-1项子集都是频繁的。可以想象一个产生3-项集的例子就很容易理解了。 3.2.2 支持度的计数 上一步产生了候选项集，那么如何筛选候选项集成为频繁项集呢？就需要计算其支持度与minsup的比较。 最容易想到的支持度计数方法就是通过比较每个事务与所有的候选集，并且相应的更新包含在事务中的候选项集的支持度计数。这种方法的计算方法是昂贵的。 不妨换个思路，我们先枚举出所有事务包含的项集，并且利用它们更新对应的候选项集的支持度。因为总体项集的数目比事物所包含的项集数目会大很多，所以这种方法会比之前的计算量上小很多。那么还有更好的方法吗？ 还很有，听说过HASH吗？如果是分布式的工具用得比较多的话，这个概念应该接触的还挺多的，将所有的事物以及项集全部加载入一个HASH搜索树中，这样的话，每次比较只需要比较每个HASH分支下的项集与事物即可，而不必将整个项集与事物全部组合计算支持度，yep，确实不错。 3.3 计算的复杂度 总结以上方法，发现频繁项集的产生的计算复杂度主要受一下几个方面影响： 支持度的阈值：降低支持度的阈值通常会导致更多的频繁项集 项的个数：项的个数的增多将导致计算及IO开销的增大 事务的个数：由于Apriori算法会反复扫描数据集，因此事务的个数增加会导致运行时间的增加 事务的平均宽度：事务的平均宽度将导致产生候选项集及支持度计算时考虑跟多的候选项集 4.规则的产生 如果湖绿前件活后件为空的规则化，一个包含k个项所产生的规则的个数为 2k−22^k - 22​k​​−2，规则产生的可以简单的理解为将某个项集Y划分为两个不为空的子集，如果这两个子集所产生的规则曼度置信度的阈值那么一个规则就由此产生了。 4.1 基于置信度的剪枝 非常遗憾的是置信度不像支持度具有单调性的性质，但是，还是有希望的，如下定理可以帮助我们： 如果规则 X→Y−XX \\rightarrow Y-XX→Y−X不满足置信度阈值，那么形如 X′→Y−X′ X^{'} \\rightarrow Y-X^{'}X​​′​​​​→Y−X​​′​​​​的规则也一定不满足置信度的阈值，其中X′X^{'}X​​′​​​​是XXX的子集。 如何解释这个定理呢？首先列出置信度的概念，规则X→Y−XX \\rightarrow Y-XX→Y−X的置信度为σ(Y)/σ(X)\\sigma(Y)/\\sigma(X)σ(Y)/σ(X)，而X′→Y−X′ X^{'} \\rightarrow Y-X^{'}X​​′​​​​→Y−X​​′​​​​的置信度为σ(Y)/σ(X′)\\sigma(Y)/\\sigma(X^{'})σ(Y)/σ(X​​′​​​​)，注意σ()\\sigma()σ()的定义为该项集所被包含的事务的个数，因此σ(X′)>=σ(X)\\sigma(X^{'})>= \\sigma(X)σ(X​​′​​​​)>=σ(X)，因此X→Y−XX \\rightarrow Y-XX→Y−X的置信度一定是大于X′→Y−X′ X^{'} \\rightarrow Y-X^{'}X​​′​​​​→Y−X​​′​​​​的。 4.2 Apriori算法中规则的产生 Apriori算法通过一种逐层的方法来产生候选规则，每层对应于规则中后件中的项数。一开始，提取的规则的后件中只包含一个项，通过将所有规则与阈值比较，过滤规则，然后再使用这些规则来产生新的候选规则。如果某个结点具有低置信度，根据4.1所阐述的定理进行剪枝。举例说明如果abc→d{abc} \\rightarrow {d}abc→d具有低置信度，那么ab→cd{ab} \\rightarrow {cd}ab→cd、ac→bd{ac} \\rightarrow {bd}ac→bd、bc→ad{bc} \\rightarrow {ad}bc→ad、a→bcd{a} \\rightarrow {bcd}a→bcd、b→acd{b} \\rightarrow {acd}b→acd、c→abd{c} \\rightarrow {abd}c→abd就可以直接剪掉。 Copyright © Niu 2018 all right reserved，powered by GitbookUpdate Time 2018-05-16 16:14:44 "},"shu-ju-wa-jue-dao-lun/ju-lei-fen-xi.html":{"url":"shu-ju-wa-jue-dao-lun/ju-lei-fen-xi.html","title":"聚类分析","keywords":"","body":"1.聚类概述 聚类是什么做什么的呢？ 简单通俗的说就是将数据分成有意义或有用的组（簇），能够反映出数据的自然结构。聚类很多情况下都是其他问题的一个起点、或者说是一种准备工作。生物学中的界门纲目科属种就是一种层次化聚类，将杂乱无章的客户信息分成不同的组以实施不同的促销手段都是聚类的应用。聚类的作用大体来说分为： 汇总。常见的汇总比方说PCA、SVD，但是面对数据特别复杂的时候是无法发挥作用的，所以需要简便的工具，例如聚类方法。 压缩。这个技术常常用于图像、声音、视频的处理，他们数据特点都是：数据之间相似度较高；某些信息的丢失是可以接受的；希望发幅度压缩数据。 有效发现最邻近。先找到邻近的簇，然后再从邻近的簇中选择邻近的点。 前面通俗的说了聚类的定义，具体来说就是仅根据在数据中发现的描述对象以及其关系信息，将数据对象分组。目标就是将使得组内的相似性很大，而组间的相似性很小。 聚类的类型是多种多样的，有一些定义需要区分： 层次的与划分的。 划分聚类，即簇的集合是非嵌套的，每个数据恰好在一个组中，例如K-means。层次聚类的簇集合是嵌套的，组成一个树（或者叫谱系图？），层次聚类可以看做划分聚类的序列，划分聚类可以通过取特定层得到。 互斥的、重叠与模糊的。 互斥，即每个对象只能指派到一个簇中。而重叠和模糊相反。重叠的现实意义是存在的，例如一个人可能是老师也可能是学生，因此需要将他指派到两个簇中。而模糊就是数学上模糊集的概念，每个数据对象不是一定属于或者不属于某个簇，而是以一定的概率分配给每个簇，但是实践中通过将对象指派到概率或者说隶属权值最高的簇中，因此模糊聚类也就转化成了互斥聚类。 完全的与部分的。 完全聚类，是指每个对象都能指派到一个簇中，而部分聚类相反。部分聚类的存在是因为为了舍弃一些噪声点、离群点或不感兴趣的点。 同时簇的类型也是多样的，不同的聚类方式使用的簇的类型也是不同的。粗略来说簇的类型大致可以分为：明显分离的，基于原型的，基于图的（连通分支、基于邻近的簇），基于密度的，共同性质的（概念簇）。 2.K-means Okay，废话了这么多，我们就来说说具体的方法吧，先从最熟悉的K-means说起。 K均值算法可以大概描述为以下几个步骤： 选择K个初始质心，K是需要人工选择的一个参数，就是所期望的簇的个数。 每个点指派到最近的质心，指派到同一质心的点形成同一个簇。 对上一步形成的每个簇计算新的质心。 重复第2、第3步，直到簇或者质心不发生变化。 K均值总是会收敛到一个解，这是不同担心的，而且收敛通常发成在早期阶段，所以放宽一下第4步的要求，例如只有百分之一的点发成变化即可。 下面我们将每一步拆开详细的来说。 2.1 选择初始质心 K均值最大的一个问题，就是初始质心的选择会导致不一样的结果，而且某些结果效果可能会非常差。常见的方法就是随机的选择质心，这种方法就不是很好的一个选择。通常的处理方法也很简单：多运行几次，每次随机选取不同的初始质心，然后最后选择具有最小SSE的簇。这个方法虽然简单，但是效果可能一般，取决于我们的数据集以及簇的个数。 那么我们可以通过使用层次化聚类对他进行改进，首先对样本进行层次化聚类，然后提取K个簇，并将这些簇的质心最为初始质心。这种方法通常很有效，可是感觉很罗嗦有没有？他只能应用于样本较小，而且K相对于样本大小较小的情况。层次化聚类已经很繁琐了，而且层次化聚类已有的结果拿来再聚类，感觉不怎么实用。 另一种方式是首先随机的选取一个点（或者直接选择样本的均值作为质心），然后对每个后续的初始质心，选择离选取过的质心最远的点。这样就确保了我们的初始质心是散开的，但是有一个问题是这样很可能选取到离群点，而且计算距离最远点的开销也很大。 还有一种处理方式是K-means的改进，二分K-means方法。 还还有一种方法是使用后处理来修补所产生的簇集。 这两种方法放到后面介绍。 2.2 指派点到最近的质心 对于欧式空间中的点，我们通常使用欧几里得距离（L2L_2L​2​​）计算，对于文档通常使用余弦相似性计算，通常还有例如曼哈顿距离（L1L_1L​1​​）和Jaccard度量。 K均值的相似性度量需要比较简单的相似性度量，因为算法每一步都要重复的计算每个点的与质心的相似度。还有例如二分K均值通过减少相似度计算量来加快K均值速度。 2.3目标函数以及更新质心 聚类的目标通常用一个目标函数表示，该函数依赖于点之间，或者点到质心的邻近性。对于欧几里得空间中的数据，我们常常使用误差平方和（SSE）来作为度量聚类质量的目标函数。SSE也成为散布。使得SSE最小的质心是均值。对于文档数据通常使用余弦相似度，也称作簇的凝聚度。其最优解也是均值。而曼哈顿距离作为度量的话，最优质心是中位数。而上面说到的欧几里得距离以及余弦相似度都可以看作是Bregman散度的特例，Bregman散度的最优解都是均值。 2.4其他问题 空簇问题 K均值存在的问题之一就是空簇的问题，就是说可能所有点都没有指派到某个簇内。这种情况下，通常会选择一个替补质心，否则平方误差会加大。一种方法是选择一个距离当前任何质心最远的点，或者从具有最大SSE簇中选择一个替补质心。 离群点 使用SSE标准的时候，离群点可能会过度影响所发现的簇，所得到的质心可能没有那么有代表性，提前删除他们是比较有用的方法。当然某些情况下是不能删除离群点的，因为有些时候离群点可能是人们感兴趣的点。 那么如何识别离群点呢？提前删除是非常好的结果，如何提前检测在后来的章节会说到。还有一种方法是使用后处理的方法，例如可以记录每个点对SSE的影响，删除那些具有异乎寻常的影响点，此外还可以删除那些很小的簇，因为他们常常代表离群点。 后处理方法 一种明显降低SSE的方法是使用较大的K，然而很多情况下我们只想降低SSE，而不希望增加簇的个数，怎么办呢？一种常用的方法是交替的使用簇分裂和簇合并。 增加簇个数的方法包括（分裂）：分裂一个具有较大SSE的簇或者分裂在特定属性上具有最大标准差的簇；引进一个新的质点，通常选择离所有簇质心最远的点，另一种方法是从所有的点或者具有最高SSE的点中随机选择。 减少簇个数的方法包括（合并）：拆散一个簇，删除簇对应的质心，簇中的点重新指派到其他簇中；合并两个簇，通常选择质心距离最近的两个簇（质心法），或者选择合并后导致总SSE增加最少的簇（Ward法）。 增量地更新质心 可以在点到簇的每次指派之后，增量的更新质心，而不是在所有点都指派到簇中之后再更新。这样还可以保证不会产生空簇（因为所有的簇都是从单个点开始）。此外，如果使用增量更新还可以调整点的相对权值，增加准确率和收敛的速度，但是这样做通常相当困难（类似神经网络训练的权值）。缺点方面是，增量地更新质心肯能导致次序的依赖性，而且计算量更加复杂。 2.5 二分K均值 二分K均值的思想非常简单，为了得到K的簇，首先将所有的簇分成两个簇，然后从中选取一个簇进行二分裂，然后在从所有的簇中选择一个进行二分裂，知道生成K个簇。 待分裂簇的选择有许多种方法，一个是选择最大的簇，或者选择具有最大SSE的簇，或者选择一个基于大小和SSE的综合指标进行选择。K均值可以得到使总SSE局部最小的聚类，而二分K均值是不能保证的，因为它只是局部的使用了K均值算法。 前面也提到过了，二分K均值的有点就是不会受初始值选定的影响，而且速度更快。而且我们通常是使用已经聚类好的结果簇作为基本K均值的质心，对结果进行逐步求精。 2.6 K均值的适用簇类型 前面说过簇的类型分为很多种，K均值对于非球形形状或具有不同尺寸或密度的簇时，很难检测到“自然的”簇。K均值的目标函数是最小化等尺寸和等密度的球形簇，或者明显分离的簇，但是如果可以接受将一个自然簇分割成若干个子簇的话，这些局限性可以在某种意义上克服。 2.7 K-means优缺点 K均值简单而且可以用于各种类型的数据，也相当有效（毕竟是最古老最广泛使用的聚类方法），而且K均值的某些变种甚至更加有效。K均值不适合处理那些非球形簇、不同尺寸、不同密度的簇，而且对包含离群点的数据进行聚类是，K均值也是有问题的。 3.凝聚层次聚类 多元学过系统聚类，这就是我们所说的凝聚层次聚类。 层次聚类通常分为两种：凝聚的，从点作为个体簇开始，每一步合并两个最接近的簇，直到所有点合并成为一个簇；分裂的，从包含所有点的簇开始，每一步分裂一个簇，直到仅剩下单点簇。 这里先说凝聚层次聚类。凝聚层次聚类通常使用树状图（谱系图）显示，对于二维点的集合也可以使用嵌套图表示。 3.1 基本凝聚层次聚类算法 从个体点作为簇开始，一次合并两个最接近的簇，直到最后只剩下一个簇。 那么这里的关键就是如何定义两个簇之间的邻近度。系统聚类里面说了8种方法，这里简单的说一下其中三个最简单的： 最长距离法（MAX，全链）利用两个簇中两个最远的点之间的邻近度作为簇的邻近度，即不同结点子集中两个结点之间的最长边。完全链接对于噪声点和离群点不太敏感，但是可能会使大的簇破裂，并且偏好球形簇(无法理解后两条，有高见的同学请帮理解一下)。 最短距离法（MIN，单链）定义簇的邻近度为两个簇中距离最近的两个点之前的邻近度。单链技术擅长处理非椭圆形的簇，对噪声和离群点很敏感。 平均距离法（组平均，group average）定义簇的邻近度为不同簇所有点对邻近度的平均值。 此外，还有两种方法，是基于原型的观点，簇用质心代表，使用质心之间的邻近度作为簇的邻近度。这个叫做质心法，质心法看上去K均值相似，其实更相似的是下面的Ward法，质心法的一个特点是：倒置的可能性，即被合并的簇可能比前一步合并的簇更加相似，对于其他方法被合并的簇之间的邻近度随着层次聚类的进展单调的增加（或者不增加）。因此这种特性通常被视作一种缺点。 另一种基于原型的方法是Ward方法，使用合并两个簇导致的SSE增加来度量两个簇之间的邻近度。该方法与K均值使用的目标函数相同，而且如果两个点之间的邻近度使用它们之间距离的平方计算时，Ward方法与组平均法十分相似。 3.2 优缺点 层次聚类可以产生层次结构，这可能正是某些数据所需要的结构，而且某些研究表明（...书中原话，感觉十分不靠谱），层次聚类可以产生高质量的聚类。 缺点来来来，一个个列出来： 缺乏全局目标函数。层次聚类在每一步局部的使用一些标准来决定哪些簇应当合并，不能全局优化。但是这样做可以劈开解决困难的组合优化问题，而且解决了局部最小问题以及初始点的选择问题。 合并是最终的。一旦两个簇进行合并，就不能撤消了。这种形式阻碍了局部最优变成全局最优。尽管如此，很多时候Ward还是作为一种初始化K均值聚类的鲁棒方法使用。解决这个问题的方法是：移动树的分支以改善全局目标函数，或者使用划分聚类技术（K均值）来创建许多小簇，然后从这些小簇出发进行层次聚类。 计算量与存储量的代价是昂贵的。层次化聚类需要计算距离矩阵（期末考试手动计算...简直惨无人道），而且还要存储这个距离矩阵，所以面对数据量较大的情况，层次化聚类通常是乏力的。 对于噪声、高维数据的处理能力差。因为所有合并都是最终的。 对于2、3、4问题都可以使用K均值进行部分聚类，在某种程度上可以解决这三个问题。 4.DBSCAN Copyright © Niu 2018 all right reserved，powered by GitbookUpdate Time 2018-05-16 15:14:29 "},"macpei-zhi-si-ren-zhi-nan.html":{"url":"macpei-zhi-si-ren-zhi-nan.html","title":"MAC配置私人指南","keywords":"","body":"MAC配置指南 APP LIST gitbook 个人资料积累及wiki系统 dash 代码查询工具 iterm 终端查询工具 xmind zen 思维导图工具，结构化资料 sublime 文本编辑器 alfred 更好用的spotligh focus timer 番茄工具发工具 mpv 视频工具 evernote 资料积累工具 eudic 字典 anyconnect vpn链接 karabiner 外接键盘神器 noizio 白噪声工具 calibre 文本格式转换工具 SUBLIME PACKAGE Alignment Color Scheme AutoFileName BracketHighlighter Codecs33 ConvertToUTF8 Emmet File History FileDiffs FileHeader Markdown Extended MarkdownEditing MarkdownTOC OmniMarkupPreviewer SideBarEnhancements SublimeCodeIntel SublimeLinter Copyright © Niu 2018 all right reserved，powered by GitbookUpdate Time 2018-05-18 17:17:59 "}}